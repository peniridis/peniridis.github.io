<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hexo+NexT v6.4.1博客优化日志]]></title>
    <url>%2Fhexo-00-major.html</url>
    <content type="text"><![CDATA[记录Hexo优化及主题NexT美化@toc主题插件「盘古之白」為什麼你們就是不能加個空格呢？by pangu.js「盤古之白」，它劈開了全形字和半形字之間的混沌。另有研究顯示，打字的時候不喜歡在中文和英文之間加空格的人，感情路都走得很辛苦，有七成的比例會在 34 歲的時候跟自己不愛的人結婚，而其餘三成的人最後只能把遺產留給自己的貓。畢竟愛情跟書寫都需要適時地留白。與大家共勉之。12$ cd themes/next$ git clone https://github.com/theme-next/theme-next-pangu.git source/lib/pangu_themes/next/_config.yaml_修改12345# Pangu Support# Dependencies: https://github.com/theme-next/theme-next-pangu# For more information: https://github.com/vinta/pangu.js# pangu: falsepangu: true静态资源压缩常规的做法是使用gulp来进行压缩，gulp是Node.js下的自动构建工具，通过一列的task执行步骤进行自动流程化处理。这里使用hexo-neat压缩插件，配置简单，可以自动完成静态资源的压缩1$ npm install hexo-neat --save添加以下配置到站点配置文件_config.yml的末尾12345678910111213141516171819202122# hexo-neat# 博文压缩neat_enable: true# 压缩htmlneat_html: enable: true exclude:# 压缩css neat_css: enable: true exclude: - &apos;**/*.min.css&apos;# 压缩jsneat_js: enable: true mangle: true output: compress: exclude: - &apos;**/*.min.js&apos; - &apos;**/jquery.fancybox.pack.js&apos; - &apos;**/index.js&apos;]]></content>
      <tags>
        <tag>Hexo</tag>
        <tag>NexT</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache HBase ™ Reference Guide：Hbase官方指南3.0（一）「数据模型」]]></title>
    <url>%2Fhbase-001-data-model.html</url>
    <content type="text"><![CDATA[校验Hbase官方指南文档Apache HBase ™ Reference Guide Version 3.0.0-SNAPSHOTData Model部分No.20到No.32小节@toc数据模型在HBase中，数据存储在具有行和列的表中。关系型数据库（RDBMS）中有相似的概念，这并不是有用的类比。不过，HBase可以被认为是一个多维度的数据映射存储。HBase Data Model Terminology Hbase（数据模型术语）Table（表）HBase表由多行组成。Row（列）HBase中的一行由一个行键和一个或多个具有与之关联的值的列组成。行存储时，行按字母顺序排序。因此，行的key的设计就显得非常重要。数据的存储目标是相近的数据存储到一起。一个常用的行的key的格式是网站域名。如果你的行的key是域名，你应该将域名进行反转(org.apache.www, org.apache.mail, org.apache.jira)再存储。这样的话，所有Apache域名将会存储在一起，而不是基于子域名的首字母分散在各处。Column（列）HBase中的列包含用「:」(冒号)分隔开的列族和列的限定符。Column Family（列族）因为性能的原因，列族物理上包含一组列和它们的值。每一个列族拥有一系列的存储属性，例如值是否缓存在内存中，数据是否要压缩或者他的行key是否要加密等等。表格中的每一行拥有相同的列族，尽管一个给定的行可能没有存储任何数据在一个给定的列族中。Column Qualifier（列限定符）列的限定符是列族中数据的索引。例如给定了一个列族content，那么限定符可能是content:html，也可以是content:pdf。列族在创建表格时是确定的了，但是列的限定符是可变的，并且行之间可能有很大差异。Cell（单元）单元是由行、列族、列限定符、值和代表值版本的时间戳组成的。Timestamp（时间戳）时间戳是写在值旁边的一个用于区分值的版本的数据。默认情况下，时间戳表示的是当数据写入时RegionSever的时间点，但你也可以在写入数据时指定一个不同的时间戳。概念视图你可以读一下 Jim R写的Understanding HBase and BigTable 博客来简单了解一下HBase的数据模型，另一个好的理解是Amandeep Khurana.的 Introduction to Basic Schema Design 。学习不同的方面的资料可能会帮助你更透彻地了解HBase的设计。所链接的文章覆盖本部分所讲的信息。接下来的例子是 取自BigTable 中第二页中的例子，在此基础上做了些许的改变。一个名为webable的表格，表格中有两行（com.cnn.www 和 com.example.www）和三个列族（contents, anchor, 和 people）。在这个例子当中，第一行(com.cnn.www)中anchor包含两列（anchor:cssnsi.com, anchor:my.look.ca）和content包含一列（contents:html）。这个例子中com.cnn.www拥有5个版本而com.example.www有一个版本。contents:html列中包含给定网页的整个HTML。anchor限定符包含能够表示行的站点以及链接中文本。People列族表示跟站点有关的人。列名按照所定义好的，一个列名的格式为列族名前缀加限定符。例如，列contents:html由列族contents和html限定符。冒号「:」用于将列族和列限定符分开。Table 6. Table webtable行键时间戳列族contents列族anchor列族people“com.cnn.www”t9anchor:cnnsi.com = “CNN”“com.cnn.www”t8anchor:cnnsi.com = “CNN”“com.cnn.www”t6contents:html = “&lt;html&gt;…​”“com.cnn.www”t5contents:html = “&lt;html&gt;…​”“com.cnn.www”t3contents:html = “&lt;html&gt;…​”“com.example.www”t5contents:html = “&lt;html&gt;…​”people:author = “John Doe”在HBase中，表格中的单元如果是空将不占用空间或者事实上不存在。这就使得HBase看起来“稀疏”。表格视图不是唯一方式来查看HBase中数据，甚至不是最精确的。下面的方式以多维度映射的方式来表达相同的信息。这只是一个说明示例的模型可能不是严格准确的。1234567891011121314151617181920212223&#123; &quot;com.cnn.www&quot;: &#123; contents: &#123; t6: contents:html: &quot;&lt;html&gt;...&quot; t5: contents:html: &quot;&lt;html&gt;...&quot; t3: contents:html: &quot;&lt;html&gt;...&quot; &#125; anchor: &#123; t9: anchor:cnnsi.com = &quot;CNN&quot; t8: anchor:my.look.ca = &quot;CNN.com&quot; &#125; people: &#123;&#125; &#125; &quot;com.example.www&quot;: &#123; contents: &#123; t5: contents:html: &quot;&lt;html&gt;...&quot; &#125; anchor: &#123;&#125; people: &#123; t5: people:author: &quot;John Doe&quot; &#125; &#125;&#125;物理视图尽管在概念层次，表可能看起来是由一些列稀疏的行组成，但他们是通过列族来存储的。一个新建的限定符(column_family:column_qualifier)可以随时地添加到已存在的列族中。Table 7. 列族 anchor行键时间戳列族anchor“com.cnn.www”t9anchor:cnnsi.com = “CNN”“com.cnn.www”t8anchor:my.look.ca = “CNN.com”Table 8. 列族 contents行键时间戳列族contents“com.cnn.www”t6contents:html = “&lt;html&gt;…​”“com.cnn.www”t5contents:html = “&lt;html&gt;…​”“com.cnn.www”t3contents:html = “&lt;html&gt;…​”概念视图中的空单元实际上是没有进行存储的。因此对于返回时间戳为t8的contents:html的值的请求，结果为空。同样的，一个返回时间戳为t9的anchor:my.look.ca的值的请求，结果也为空。然而，如果没有指定时间戳的话，那么会返回特定列的最新值。对有多个版本的列，优先返回最新的值，因为时间戳是按照递减顺序存储的。因此对于一个返回com.cnn.www里面所有的列的值并且没有指定时间戳的请求，返回的结果会是时间戳为t6的contents:html的值、时间戳 t9的anchor:cnnsi.com的值和时间戳t8的 anchor:my.look.ca 。关于Apache Hbase如何存储数据的内部细节，请查看 regions.arch.命名空间命名空间是一个类似于关系型数据库系统中的数据库的逻辑上的表分组的概念。这个抽象的概念为即将到来的多租户相关特性奠定了基础：配额管理 (HBASE-8410) - 限制命名空间可以使用的资源量（即区域，表）。命名空间安全管理 (HBASE-9206) - 为租户提供另一级别的安全管理。区域服务器组 (HBASE-6721) - 可以将命名空间/表固定到RegionServers的子集上，从而保证粗略的隔离级别。命名空间管理命名空间可以被创建、移除和修改。命名空间关系的指定是在创建表格通过指定一个完全限定表名的形式完成的：1&lt;table namespace&gt;:&lt;table qualifier&gt;Example 7. Examples1234567891011#Create a namespacecreate_namespace &apos;my_ns&apos;#create my_table in my_ns namespacecreate &apos;my_ns:my_table&apos;, &apos;fam&apos;#drop namespacedrop_namespace &apos;my_ns&apos;#alter namespacealter_namespace &apos;my_ns&apos;, &#123;METHOD =&gt; &apos;set&apos;, &apos;PROPERTY_NAME&apos; =&gt; &apos;PROPERTY_VALUE&apos;&#125;预定义命名空间有两种预定义的特殊的命名空间hbase – 系统命名空间, 用于包含HBase内部表default – 没有明确指定命名空间的表将会自动落入这个命名空间Example 8. Examples12345#namespace=foo and table qualifier=barcreate &apos;foo:bar&apos;, &apos;fam&apos;#namespace=default and table qualifier=barcreate &apos;bar&apos;, &apos;fam&apos;表在Schema（模式）定义时预先声明表。行行键是未解释的字节。行是按照字典顺序进行排序的并且最小的排在前面。空的字节数据用来表示表格的命名空间的开头和结尾。列族列在HBase中是归入到列族里面的。一个列族的所有列成员都有相同的前缀。例如，列courses:history和cources:math是cources列族的成员，冒号用于将列族和列限定符分开。列族前缀必须由可打印输出的字符组成。列限定符可以由任意字节组成。列族必须在结构定义阶段预先声明好，而列则不需要在结构设计阶段预先定义，而是可以在表的创建和运行阶段动态生成。物理上来说，所有的列族成员都是存储在文件系统。因为调试和存储参数都是在列族级别完成，建议所有的列族都要拥有相同的访问模式和大小特征。单元一个{row,column,version}完全指定了HBase的一个单元。单元内容是未解释的字节数据模型操作数据模型的四个主要操作是Get，Put，Scan和Delete。可以通过Table实例进行操作。GetGet 返回指定行的属性 Gets 通过 Table.get).执行。PutPut 操作是在行键不存在时添加新行或者行键已经存在时进行更新。 Puts 是通过 Table.put) (写缓存) 或者Table.batch) (没有写缓存)执行的。ScansScan 允许为指定属性迭代多行。下面是表格实例中Scan的例子。假设一个表格里面有”row1”, “row2”, “row3”，然后有另外一组行键为”abc1”, “abc2”,和”abc3”。下面的例子展示如何设置一个Scan实例来返回以“row”开头的行。123456789101112131415161718192021222324252627282930313233public static final byte[] CF = "cf".getBytes(); public static final byte[] ATTR = "attr".getBytes(); ... Table table = ... // instantiate a Table instance Scan scan = new Scan(); scan.addColumn(CF, ATTR); scan.setRowPrefixFilter(Bytes.toBytes("row")); ResultScanner rs = table.getScanner(scan); try &#123; for (Result r = rs.next(); r != null; r = rs.next()) &#123; // process result... &#125; &#125; finally &#123; rs.close(); // always close the ResultScanner! &#125;需要说明的是通常最简单的指定Scan的一个特定停止点的方法是使用InclusiveStopFilter 类。DeleteDelete 操作是将一个行从表中移除. Deletes 通过 Table.delete)执行。HBase不会立刻对数据的进行操作（可以理解为不对数据执行删除操作），而是为死亡数据创建一个称为墓碑的标签。这个墓碑和死亡数据会在重要精简工作中被删除。查看 version.delete 获取更多关于列的版本删除的信息，查看 compaction 获取关于压缩工作的更多信息。版本A {row, column, version} 在HBase完全指定一个单元。理论上来说行和列都一样的单元的数量是无限的，因为单元的地址是通过版本这个维度来区分的。行和列使用字节来表达，而版本是通过长整型来指定的。典型来说，这个长时间实例就像java.util.Date.getTime() 或者 System.currentTimeMillis()返回的一样，以毫秒为单位，返回当前时间和 January 1, 1970 UTC的时间差 。HBase的版本维度以递减顺序存储，以致读取一个存储的文件时，返回的是最新版本的数据。关于单元的版本有许多的困扰，尤其是：如果多个数据写到一个具有相同版本的单元里，只能获取到最后写入的那个以非递增的版本顺序写入也是可以的。下面我们将描述HBase中版本维度是如何运作的。可以看 HBASE-2406 关于HBase版本的讨论。 Bending time in HBase 是关于HBase的版本或者时间维度的好读物。它提供了比这里更多的关于版本的细节信息。正如这里写到的，这里提到的覆盖存在的时间戳的限制将不再存在。这部分只是Bruno Dumon所写的关于版本的基本大纲。指定版本的存储数量版本的最大存储数量是列结构的一个部分并且在表格创建时指定，或者通过alter命令行，或者通过 HColumnDescriptor.DEFAULT_VERSIONS来修改。HBase0.96之前，默认数量是3，HBase0.96之后改为1.Example 13. 修改一个列族的最大版本数这个例子使用HBase Shell来修改列族 f1的最大版本数为5，你也可以使用 HColumnDescriptor来实现。1hbase&gt; alter ‘t1′, NAME =&gt; ‘f1′, VERSIONS =&gt; 5Example 14. 修改一个列族的最小版本数Modify你也可以通过指定最小半本书来存储列族。默认情况下，该值为零，意味着这个属性是禁用的。下面的例子是通过HBase Shell设置列族f1中的所有列的最小版本数为2。你也可以通过 HColumnDescriptor来实现。1hbase&gt; alter ‘t1′, NAME =&gt; ‘f1′, MIN_VERSIONS =&gt; 2从HBase0.98.2开始，你可以通过设定在hbase-site.xml中设置hbase.column.max.version属性为所有新建的列指定一个全局的默认的最大版本数。版本和HBase 操作在这部分我们来看一下版本维度在HBase的每个核心操作中的表现。Get/ScanGet是通过获取Scan的第一个数据来实现的。下面的讨论适用于 Get 和 Scans.。默认情况下，如果你没有指定明确的版本，当你执行一个Get操作时，那个版本为最大值的单元将被返回（可能是也可能不是最新写人的那个）。默认的行为可以通过下面方式来修改：返回不止一个版本 查看 Get.setMaxVersions())返回最新版本以外的版本, 查看 Get.setTimeRange())想要获得小于或等于固定值的最新版本，仅仅通过使用一个0到期望版本的范围和设置最大版本数为1就可以实现获得一个特定时间点的最新版本的记录。默认的Get 例子下面例子仅仅返回行的当前版本。1234567891011public static final byte[] CF = "cf".getBytes(); public static final byte[] ATTR = "attr".getBytes(); ... Get get = new Get(Bytes.toBytes("row1")); Result r = table.get(get); byte[] b = r.getValue(CF, ATTR); // returns current version of valueGet版本的例下面是获得行的最新3个版本的例子：123456789101112131415public static final byte[] CF = "cf".getBytes(); public static final byte[] ATTR = "attr".getBytes(); ... Get get = new Get(Bytes.toBytes("row1")); get.setMaxVersions(3); // will return last 3 versions of row Result r = table.get(get); byte[] b = r.getValue(CF, ATTR); // returns current version of value List&lt;KeyValue&gt; kv = r.getColumn(CF, ATTR); // returns all versions of this columnPutPut操作常常是以固定的时间戳来创建一个新单元。默认情况下，系统使用服务的 currentTimeMillis，但是你也可以为每一个列自己指定版本（长整型）。这就意味着你可以指定一个过去或者未来的时间点，或者不是时间格式的长整型。为了覆盖已经存在的值，对和那个你想要覆盖的单元完全一样的row、column和version进行put操作。隐式版本例子下面Put是以当前时间为版本的隐式操作1234567891011public static final byte[] CF = "cf".getBytes(); public static final byte[] ATTR = "attr".getBytes(); ... Put put = new Put(Bytes.toBytes(row)); put.add(CF, ATTR, Bytes.toBytes( data)); table.put(put);显示版本例子下面的put是显示指定时间戳的操作。12345678910111213public static final byte[] CF = "cf".getBytes(); public static final byte[] ATTR = "attr".getBytes(); ... Put put = new Put( Bytes.toBytes(row)); long explicitTimeInMs = 555; // just an example put.add(CF, ATTR, explicitTimeInMs, Bytes.toBytes(data)); table.put(put);警告: 版本时间戳是HBase内部用来计算数据的存活时间的。它最好避免自己设置。最好是将时间戳作为行的单独属性或者作为key的一部分，或者两者都有。DeleteThere are three different types of internal delete markers. See Lars Hofhansl’s blog for discussion of his attempt adding another, Scanning in HBase: Prefix Delete Marker.有三种不同的删除类型。可以看看Lars Hofhansl所写的博客 Scanning in HBase: Prefix Delete Marker.Delete:列的指定版本Delete column:列的所有版本Delete family:特定列族里面的所有列。当要删除整个行时，HBase将会在内部为每一个列族创建一个墓碑。删除通过创建一个墓碑标签来工作的。例如，让我们来设想我们要删除一个行。为此你可指定一个版本，或者使用默认的currentTimeMillis 。这就是删除小于等于该版本的所有单元。HBase不会修改数据，例如删除操作将不会立刻删除满足删除条件的文件。相反的，称为墓碑的会被写入，用来掩饰被删除的数据。当HBase执行一个压缩操作，墓碑将会执行一个真正地删除死亡值和墓碑自己的删除操作。如果你的删除操作指定的版本大于目前所有的版本，那么可以认为是删除整个行的数据。你可以在 Put w/timestamp → Deleteall → Put w/ timestamp fails 用户邮件列表中查看关于删除和版本之间的相互影响的有益信息。keyvalue也可以到keyvalue 查看更多关于内部KeyValue格式的信息。删除标签会在下一次仓库压缩操作中被清理掉，除非为列族设置了 KEEP_DELETED_CELLS (查看 Keeping Deleted Cells)。为了保证删除时间的可配置性，你可以通过在 hbase-site.xml 。中hbase.hstore.time.to.purge.deletes属性来设置TTL（生存时间）。如果 hbase.hstore.time.to.purge.deletes没有设置或者设置为0，所有的删除标签包括哪些墓碑都会在下一次精简操作中被干掉。此外，未来带有时间戳的删除标签将会保持到发生在hbase.hstore.time.to.purge.deletes加上代表标签的时间戳的时间和的下一次精简操作。This behavior represents a fix for an unexpected change that was introduced in HBase 0.94, and was fixed in HBASE-10118. The change has been backported to HBase 0.94 and newer branches.Optional New Version and Delete behavior in HBase-2.0.0In hbase-2.0.0, the operator can specify an alternate version and delete treatment by setting the column descriptor propertyNEW_VERSION_BEHAVIOR to true (To set a property on a column family descriptor, you must first disable the table and then alter the column family descriptor; see Keeping Deleted Cells for an example of editing an attribute on a column family descriptor).The ‘new version behavior’, undoes the limitations listed below whereby a Delete ALWAYS overshadows a Put if at the same location — i.e. same row, column family, qualifier and timestamp — regardless of which arrived first. Version accounting is also changed as deleted versions are considered toward total version count. This is done to ensure results are not changed should a major compaction intercede. See HBASE-15968 and linked issues for discussion.Running with this new configuration currently costs; we factor the Cell MVCC on every compare so we burn more CPU. The slow down will depend. In testing we’ve seen between 0% and 25% degradation.If replicating, it is advised that you run with the new serial replication feature (See HBASE-9465; the serial replication feature did NOT make it into hbase-2.0.0 but should arrive in a subsequent hbase-2.x release) as now the order in which Mutations arrive is a factor.当前的局限性Deletes mask Puts删除覆盖插入/更新删除操作覆盖插入/更新操作，即使put在delete之后执行的。可以查看 HBASE-2256. 还记得一个删除写入一个墓碑，只有当下一次精简操作发生时才会执行真正地删除操作。假设你执行了一个删除全部小于等于T的操作。在此之外又做了一个时间戳为T的put操作。这个put操作即使是发生在delete之后，也会被delete墓碑所覆盖。执行put的时候不会报错，不过当你执行一个get的时候会发现执行无效。你会在精简操作之后重新开始工作。如果你在put的使用的递增的版本，那么这些问题将不会出现。但如果你不在意时间，在执行delelte后立刻执行put的话，那么它们将有可能发生在同一时间点，这将会导致上述问题的出现。精简操作影响查询结果创建三个版本为t1,t2,t3的单元，并且设置最大版本数为2.所以当我们查询所有版本时，只会返回t2和t3。但是当你删除版本t2和t3的时候，版本t1会重新出现。显然，一旦重要精简工作运行之后，这样的行为就不会再出现。（查看 Bendingtime in HBase.）排序次序HBase中所有的数据模型操作返回的数据都是经过排序的。首先是行排序，其次是列族，接着是列限定符，最后是时间戳（递减排序，左右最新的记录最先返回）列元数据所有列的元数据都存储在一个列族的内部KeyValue实例中。因此，HBsase不仅支持一行中有多列，而且支持行之间的列的差异多样化。跟踪列名是你的责任。唯一获取一个列族的所有列的方法是处理所有的行。查看 keyvalue获得更多关于HBase内部如何存储数据的信息。JoinsHBase是否支持join是一个常见的问题，答案是没有，至少没办法像RDBMS那样支持（例如等价式join或者外部join）。正如本章节所阐述的，HBase中读取数据的操作是Get和Scan。然而，这不意味着等价式join功能没办法在你的应用中实现，但是你必须自己实现。两种主要策略是将数据非结构化地写到HBase中，或者查找表格然后在应用中或者MapReduce代码中实现join操作（正如RDBMS所演示的，将根据表格的大小会有几种不同的策略，例如嵌套使循环和hash-join）。哪个是最好的方法？这将依赖于你想做什么，没有一种方案能够应对各种情况。ACID查看 ACID Semantics. Lars Hofhansl也写了一份报告 ACID in HBase.]]></content>
      <tags>
        <tag>Hbase</tag>
        <tag>翻译</tag>
        <tag>Guide</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软考（二）：操作系统]]></title>
    <url>%2Fsoftexam-002-os.html</url>
    <content type="text"><![CDATA[记录操作系统中知识点@toc考点进程进程的基本概念以及状态变化进程死锁进程同步、复制，信号量，前趋图，PV原语存储其他细节进程就绪状态：进程已得到运行所需资源，只等待CPU的调度便可运行；运行状态：进程已得到运行所需资源，并且得到了CPU的调度；等待状态：不具备运行条件、等待时机的状态。另：等待状态也称阻塞状态；挂起：把该进程从内存中搬到外存上。激活：又叫唤醒或恢复，操作是一样的，只是叫法不一样而已，该操作是把外存上的某个进程弄到内存上。为什么要引入挂起和激活操作呢？用户的需要。用户调试一个程序的时候，运行该程序一多半了，但是，忽然发现该程序此时有Bug，用户想停下来修改，但是修改后，用户又不想从头开始运行该程序，此为一因。操作系统的需要。操作系统管理着资源的分配，它无法忍受那些占着资源而不运行的程序，另外，这些进程也会妨碍系统的运行速度，此为一因。等等。进程的死锁进程管理师操作系统的核心，但如果设计不当，就会出现死锁的问题。如果一个进程在等待一个不可能发生的事，则进程就死锁了。而如果一个或多个进程产生死锁，就会造成系统死锁例如：系统有3个进程：A、B、C。这三个进程都需要5个系统资源。如果系统有13个资源，则不可能发生死锁。死锁发生的必要条件互斥条件：即一个资源每次只能被一个进程使用，在操作系统中这是真实存在的情况。保持和等待条件：有一个进程已经获得了一些资源，但因请求其他资源被阻塞时，对已获取的资源保持不放。不剥夺条件：有些系统不可资源是不可剥夺的，当某个进程已获得这种资源后，系统不能强行收回，只能由进程使用完时自己释放。环路等待条件：若干个进程形成环形链，每个都占用对方要申请的下一个资源。解决死锁的策略死锁预防：例如：要求用户申请资源时一起申请所需的全部资源，这就破坏了保持和等待条件；将资源分层，得到上一层资源后，才能够申请下一层资源，它破坏了环路等待条件。预防通常会降低系统的效率。死锁避免：避免是指进程在每次申请资源时判断这些操作是否安全，典型算法是「银行家算法」。但这种算法会增加系统的开销。死锁检测：死锁解除银行家算法著名的银行家算法，最早是由Dijkstra提出来的。它是一种最有代表性的避免死锁的算法。在避免死锁方法中允许进程动态地申请资源，但系资源分配之前，应先计算此次分配资源的安全性，若分配不会导致系统进入不安全状态，则分配，否则等待。银行家算法最重要的就是判断是可用资源和仍需资源之间的关系，如果可用资源数大于人需资源数，那么我们认为这个进程就是可以执行的，也是安全的，反之，便是不安全的。所以重中之重的是找到各种资源数。对进程的判断遵循以下步骤:计算系统开始时所有的资源数,即开始的可用资源数;在仍需资源数和可用资源数中作比较,找到符合条件的进程,最后修改进程执行完毕时系统的可用资源数;继续比较剩余进程和可用资源数,找到下边可以执行的进程;依次类推;【例】假设系统中有3类互斥资源R1、R2、R3，可用资源分别是9、8、5,。在T0时刻系统中有P1、P2、P3、P4和P5五个进程，这些进程对资源的最大需求量和已分配资源数如下表所示，则进程如何执行是安全的。这里需要强调的是，无论题目中给出何种条件，我们只要找到以下信息便可从容应对各种变化：【注】：可用资源：表示相应的进程执行完毕（即释放该进程占用的资源）以后可用的资源，满足公式可用资源=可用资源+已分配资源，（因为已分配的资源将会在进程执行完毕以后释放，所以可用资源会不断增多，进程执行完毕便会全部释放）同时它也是下一个进程执行时可用的资源。**需要说明的是根据进程执行情况的不同，每次填入表格中的可用资源也不会相同（因为每个进程分配的资源是有差异的），那么执行顺序也会有所差异，合理即可。仍需资源：仍需资源数=最大需求量-已分配资源数，据此公式可以求得R1、R2、R3在不同的进程时仍需的资源数，如上表中所示。按照之前所讲的步骤，实现如下：1234567R1已分配的总资源数为1+2+2+1+1=7R2已分配的总资源数为2+1+1+2+1=7R3已分配的总资源数为1+1+0+0+3=5则R1 R2 R3可用资源数分别为R1=9-7=2R2=8-7=1R3=5-5=0开始有的资源数R1 R2R3分别为2、1、0,所以从仍需资源中查找(需要说明的是查找的时候以最少资源数作为限定条件能够较快地找出结果),只有P2进程符合条件,此时可用资源变为4、2、1；接下来在在其余的进程中查找符合条件的进程,只能执行P4,此时可用资源变为5、4、1,以此类推,按照以上的步骤即可找到所有进程执行的顺序P2-&gt;P4-&gt;P5-&gt;P1-&gt;P3；以上便是有关银行家算法的计算过程。前趋图前趋图（Precedence Graph）是一个有向无循环图，记为DAG（Directed Acyclic Graph），用于描述进程之间执行的前后关系。途中的每个节点可用于描述一个程序段或进城，乃至一条语句；节点间的有向边则用于表示两个节点之间存在的偏序（Partial Order）或前驱关系（Precedence Relation）「→」如果]]></content>
      <tags>
        <tag>软考</tag>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软考（一）：文法]]></title>
    <url>%2Fsoftexam-001-grammar.html</url>
    <content type="text"><![CDATA[记录编译原理中文法的知识点编译原理顾名思义就是处理高级语言，使之称为计算机能够识别的语言（低级语言）的原理。而文法呢？就是用来描述程序设计语言的方法。类似佛法,用来描述佛家的诵经禅道的规则的。@toc概念正如英语是由句子组成的集合，而句子又是由单词和标点符号组成的序列那样。程序设计语言 C 语言，是由 C 程序所组成的集合，而程序是由类似 if , begin, end 的符号，字母和数字这样一些基本符号所组成。从字面上看，每个程序都是一个“基本符号”串，设有一基本符号串，那么 C 语言可看成是在这个基本符号集上定义的，按一定规则构成的一切基本符号串组成的集合。通俗的讲就是：根据一些指定的规则，来确定编程语言的语法，从而实现编译器的功能。终结符和非终结符文法是由非终结符（大写字母）和终结符（小写字母）以及“—&gt;”组成的。1234A—&gt; aB—&gt;dbaS—&gt; AbadB—&gt;d通过上面的几个例子可以看出：非终结符A、B、S，一般是写在左边，而终结符a、dba、b，一般是写在右边的。当然习惯的写法是非终结符用S（Start）表示，写在左边，自然而然的小写的就在右边了，这样也便于我们记忆文法的表示方式。1234567G2[S]S-&gt;ApS-&gt;BqA-&gt;aA-&gt;cAB-&gt;bb-&gt;dB如上所示：在这个推导式的集合中，存在六个推导式。其中S、A、B为非终结符。a、b、c、d、q、p为终结符。终结符是原子不可分的。分类在1956年的春天，一个叫乔姆斯基(Chomsky)的人发明了上述文法，他觉得有些文法存在着相似的形式，于是他就给文法分了一下类。首先有一个前提：设有一个组合G=(Vn,Vt,P,S)。其中Vn是非终结符的集合，Vt是终结符的集合，P是推导式的一个集合，S是开始符。就上面的例子来说，A、B、S是Vn，a、dba、b是Vt，整个的集合为P。0型文法这是最简单的一个文法。它比较宽容，没有那么多的限制条件。左边必须要包含这些元素或者元素组合中的至少一个非终结符，右边可以是这些元素的任意组合，这样就构成了0型文法。由于限制最少，所以见到的文法至少是一个0型文法。如：1A-&gt;a1型文法（上下文有关文法）1型文法也叫上下文有关文法，此文法对应于线性有界自动机。它在0型文法的基础之上，只添加了一个要求：右边的长度&gt;=左边的长度（终结符或非终结符的个数）。A—&gt; a、B—&gt;dba则是1型文法，而adB—&gt;d不符合1型文法要求 注意这里有一个特殊的形式 S—&gt; ∑（∑表示空），也是一个1型文法。2型文法（上下文无关文法）2型文法也叫上下文有关文法，此文法对应于下推自动机自动机。它在1型文法的基础上，有增加了一个要求：左边必须是非终结符（个数不限）。 如：AB—&gt;de 属于2型文法，而 Aa—&gt;DE则不是，因为Aa中含有a。3型文法（正规文法）3型文法也叫正规文法，它对应有限状态自动机。它是在2型的基础上提出了要么一个非终结符推出一个终结符，要么一个非终结符推出一个终结符并且带一个非终结符。A-&gt;a | aB（右线性）或 A-&gt;a | Ba（左线性）而上面的左线性、右线性是相互独立的。如：A—&gt;b、A—&gt;bD这是3型文法但是A—&gt;b、A—&gt;bD、A—&gt;Db则不是3型文法。从这里可以看出，对于3型文法，它不是左右线性的“组合”，要么都是右线性，要么都是左线性。三种文法关系$$3\text{型文法} \subset2\text{型文法} \subset1\text{型文法} \subset0\text{型文法}$$正规式item文法产生式正规式规则1$$A-&gt;xB,B-&gt;y$$$$A=xy$$规则2$$A-&gt;xA\vert y$$$$A=x^*y$$规则3$$A-&gt;x,A-&gt;y$$$$A=x\vert y$$实例实例1$$A-&gt;\varepsilon|aB,B-&gt;Ab|a$$判断上面推导式中满足什么类型的文法首先要判断哪些是终结符和非终结符,简单来讲终结符就是终结的,最小的不可拆分的元素,而不是终结符的都是非终结符.这个应该是没有任何问题的,所以上题中,的非终结符就是AB,其他都是非终结符.而0型文法中,讲到只需要在p中至少有一个非终结符,也就是在推导式的左边至少存在一个非终结符就可以了.这样一来,我们看到在等式的左边,两个都是非终结符.肯定满足0型文法,下面就是1型文法了,1型文法是怎么规定的呢?在0型文法的基础上,推导式的左边的长度肯定小于或者等于右边的长度,题中p集合里面左边的长度都小于右边的长度,所以肯定符合1型文法;接下来就是看看是否满足2型文法了.2型文法是怎么来限定的呢？在1型文法的基础上，在推导式的左边每个都是非终结符，如题，每个推导式的左边都是非终结符，所以肯定是2型文法了；3型文法的意思就是2型文法的基础上，看看是否满足右线性或者左线性，我们将这个推导式分开来判断。A–&gt;a或者A—&gt;aB，第一个拆开的推导式是右线性，而B—&gt;A是左线性的，3型文法是怎么规定的呢？是符合左线性或者右线性。实例2$$\begin{align}&amp;\mbox{文法}: G: S-&gt;xSx | y\mbox{所识别的语言是}（）。\&amp;A. xyx\&amp;B. (xyz)^\&amp;C. x^yx^\&amp;D. x^nyx^n(n\geq0)\\end{align*}$$解析：D。$$S-&gt;xSx-&gt;xxSxx-&gt;x…S…x-&gt;x…y…x-&gt;x^nyx^n$$因为S-&gt;y，所以 n 可以为0，即 n 的范围为大于等于0。实例312345给定文法A-&gt;bA|ca,该文法的句子是：A. bbaB. cabC. bcaD. cba解析：C。A-&gt;bA-&gt;bca。第二次替换A的时候，使用候选式A-&gt;ca 即可。实例4$$\begin{align}&amp;\text{对于一下编号为①，②，③的正则式，正确的说法是___。}\&amp;\begin{matrix}\text{①}(aa^|ab)^b &amp; \text{②}(a|b)^b &amp; \text{③}((a|b)^|aa)^b \\qquad \\end{matrix}\&amp;\begin{matrix}A. \text{正则式①，②等价} &amp; \qquad &amp; B. \text{正则式①，③等价}\C. \text{正则式②，③等价} &amp; \qquad &amp; D. \text{正则式①，②，③互不等价}\end{matrix}\end{align}$$解析：C。①中任意个前始终有b，②中为任意个a或b，故①②不等价，③中包含②并且任意长度a和故b组成的串，aa限制条件不是必要条件故②③等价。实例5$$\begin{align}&amp;\text{语言}L={a^mb^n|m\geq0,n\geq1} \text{的正规表达式是_。}\&amp;\begin{matrix}A. a^bb^ &amp; B. aa^bb^ &amp; C. aa^b^ &amp; D. a^b^\end{matrix}\end{align}$$解析：A。$a^$ 代表若干个a包括零个a，$b^b$表示至少一个b。default]]></content>
      <tags>
        <tag>软考</tag>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world.html</url>
    <content type="text"><![CDATA[github + hexo + NexTWelcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.Quick StartCreate a new post1$ hexo new "My New Post"More info: WritingRun server1$ hexo serverMore info: ServerGenerate static files1$ hexo generateMore info: GeneratingDeploy to remote sites1$ hexo deployMore info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Scrapy-cluster分布式爬虫]]></title>
    <url>%2Fpython-001-scrapy-cluster.html</url>
    <content type="text"><![CDATA[记录搭建scrapy-cluster以及管理工具scrapyd+spiderkeeper基于Scrapy-cluster库的kafka-monitor可以实现分布式爬虫Scrapyd+Spiderkeeper实现爬虫的可视化管理环境IPRole168.*.*.118Scrapy-cluster,scrapyd,spiderkeeper168.*.*.119Scrapy-cluster,scrapyd,kafka,redis,zookeeper12345678# cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core) # python -VPython 2.7.5# java -versionopenjdk version &quot;1.8.0_181&quot;OpenJDK Runtime Environment (build 1.8.0_181-b13)OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)Zookeeper 单机配置下载并配置1234567# wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz# tar -zxvf zookeeper-3.4.13.tar.gz# cd zookeeper-3.4.13/conf# cp zoo_sample.cfg zoo.cfg# cd ..# PATH=/opt/zookeeper-3.4.13/bin:$PATH# echo &apos;export PATH=/opt/zookeeper-3.4.13/bin:$PATH&apos; &gt; /etc/profile.d/zoo.sh单节点启动123456# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/zookeeper-3.4.13/bin/../conf/zoo.cfgError contacting service. It is probably not running.# zkServer.sh startkafka 单机配置下载123# wget http://mirrors.hust.edu.cn/apache/kafka/2.0.0/kafka_2.12-2.0.0.tgz# tar -zxvf kafka_2.12-2.0.0.tgz# cd kafka_2.12-2.0.0/配置123456789101112# vim config/server.properties############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.broker.id=0 # kafka的机器编号，host.name = 168.*.*.119 # 绑定ipport=9092 # 默认端口9092，# Switch to enable topic deletion or not, default value is falsedelete.topic.enable=true############################# Zookeeper #############################zookeeper.connect=localhost:2181启动1nohup bin/kafka-server-start.sh config/server.properties &amp;停止命令bin/kafka-server-stop.sh config/server.propertiesredis 单机配置安装配置123# yum -y install redis# vim /etc/redis.confbind 168.*.*.119启动1# systemctl start redis.servicescrapy-cluster 单机配置123# git clone https://github.com/istresearch/scrapy-cluster.git# cd scrapy-cluster# pip install -r requirements.txt离线运行单元测试,以确保一切似乎正常1# ./run_offline_tests.sh修改配置123# vim kafka-monitor/settings.py# vim redis-monitor/settings.py# vim crawlers/crawling/settings.py修改以下12345678910111213141516# Redis host configurationREDIS_HOST = &apos;168.*.*.119&apos;REDIS_PORT = 6379REDIS_DB = 0KAFKA_HOSTS = &apos;168.*.*.119:9092&apos;KAFKA_TOPIC_PREFIX = &apos;demo&apos;KAFKA_CONN_TIMEOUT = 5KAFKA_APPID_TOPICS = FalseKAFKA_PRODUCER_BATCH_LINGER_MS = 25 # 25 ms before flushKAFKA_PRODUCER_BUFFER_BYTES = 4 * 1024 * 1024 # 4MB before blocking# Zookeeper SettingsZOOKEEPER_ASSIGN_PATH = &apos;/scrapy-cluster/crawler/&apos;ZOOKEEPER_ID = &apos;all&apos;ZOOKEEPER_HOSTS = &apos;168.*.*.119:2181&apos;启动监听12# nohup python kafka_monitor.py run &gt;&gt; /root/scrapy-cluster/kafka-monitor/kafka_monitor.log 2&gt;&amp;1 &amp;# nohup python redis_monitor.py &gt;&gt; /root/scrapy-cluster/redis-monitor/redis_monitor.log 2&gt;&amp;1 &amp;scrapyd 爬虫管理工具配置安装1# pip install scrapyd配置12# sudo mkdir /etc/scrapyd# sudo vi /etc/scrapyd/scrapyd.conf1234567891011121314151617181920212223242526272829[scrapyd]eggs_dir = eggslogs_dir = logsitems_dir =jobs_to_keep = 5dbs_dir = dbsmax_proc = 0max_proc_per_cpu = 10finished_to_keep = 100poll_interval = 5.0bind_address = 0.0.0.0http_port = 6800debug = offrunner = scrapyd.runnerapplication = scrapyd.app.applicationlauncher = scrapyd.launcher.Launcherwebroot = scrapyd.website.Root[services]schedule.json = scrapyd.webservice.Schedulecancel.json = scrapyd.webservice.Canceladdversion.json = scrapyd.webservice.AddVersionlistprojects.json = scrapyd.webservice.ListProjectslistversions.json = scrapyd.webservice.ListVersionslistspiders.json = scrapyd.webservice.ListSpidersdelproject.json = scrapyd.webservice.DeleteProjectdelversion.json = scrapyd.webservice.DeleteVersionlistjobs.json = scrapyd.webservice.ListJobsdaemonstatus.json = scrapyd.webservice.DaemonStatus启动1# nohup scrapyd &gt;&gt; /root/scrapy-cluster/scrapyd.log 2&gt;&amp;1 &amp;建议做Nginx反向代理启动异常1234567891011121314File &quot;/usr/local/lib/python3.6/site-packages/scrapyd-1.2.0-py3.6.egg/scrapyd/app.py&quot;, line 2, in &lt;module&gt;from twisted.application.internet import TimerService, TCPServerFile &quot;/usr/local/lib64/python3.6/site-packages/twisted/application/internet.py&quot;, line 54, in &lt;module&gt;from automat import MethodicalMachineFile &quot;/usr/local/lib/python3.6/site-packages/automat/__init__.py&quot;, line 2, in &lt;module&gt;from ._methodical import MethodicalMachineFile &quot;/usr/local/lib/python3.6/site-packages/automat/_methodical.py&quot;, line 210, in &lt;module&gt; class MethodicalInput(object):File &quot;/usr/local/lib/python3.6/site-packages/automat/_methodical.py&quot;, line 220, in MethodicalInput @argSpec.defaultbuiltins.TypeError: &apos;_Nothing&apos; object is not callableFailed to load application: &apos;_Nothing&apos; object is not callable解决：Automat降级1pip install Automat==0.6.0Spiderkeeper 爬虫管理界面配置安装1pip install SpiderKeeper启动12mkdir /root/spiderkeeper/nohup spiderkeeper --server=http://168.*.*.118:6800 --username=admin --password=admin --database-url=sqlite:////root/spiderkeeper/SpiderKeeper.db &gt;&gt; /root/scrapy-cluster/spiderkeeper.log 2&gt;&amp;1 &amp;浏览器访问http://168.*.*.118:5000使用Spiderkeeper 管理爬虫使用scrapyd-deploy部署爬虫项目修改scrapy.cfg配置1vim /root/scrapy-cluster/crawler/scrapy.cfg123456[settings]default = crawling.settings[deploy]url = http://168.*.*.118:6800/project = crawling添加新的spider1cd /root/scrapy-cluster/crawler/crawling/spider使用scrapyd-deploy部署项目123456# cd /root/scrapy-cluster/crawler# scrapyd-deploy Packing version 1536225989Deploying to project &quot;crawling&quot; in http://168.*.*.118:6800/addversion.jsonServer response (200):&#123;&quot;status&quot;: &quot;ok&quot;, &quot;project&quot;: &quot;crawling&quot;, &quot;version&quot;: &quot;1536225989&quot;, &quot;spiders&quot;: 3, &quot;node_name&quot;: &quot;ambari&quot;&#125;spiderkeeper 配置爬虫项目登录Spiderkeeper创建项目使用scrapy.cfg中配置的项目名创建后再Spiders-&gt;Dashboard中看到所有spiderScrapy-cluster 分布式爬虫Scrapy Cluster需要在不同的爬虫服务器之间进行协调，以确保最大的内容吞吐量，同时控制集群服务器爬取网站的速度。Scrapy Cluster提供了两种主要策略来控制爬虫对不同域名的攻击速度。这由爬虫的类型与IP地址确定，但他们都作用于不同的域名队列。Scrapy-cluster分布式爬虫，分发网址是基于IP地址。在不同的机器上启动集群，不同服务器上的每个爬虫去除队列中的所有链接。部署集群中第二个scrapy-cluster配置一台新的服务器参照scrapy-cluster 单机配置,同时使用第一台服务器配置kafka-monitor/settings.py redis-monitor/settings.py crawling/settings.pyCurrent public ip 问题由于两台服务器同时部署在相同内网，spider运行后即获取相同Current public ip，导致scrapy-cluster调度器无法根据IP分发链接12018-09-07 16:08:29,684 [sc-crawler] DEBUG: Current public ip: b&apos;110.90.122.1&apos;参考代码/root/scrapy-cluster/crawler/crawling/distributed_scheduler.py第282行：1234567891011121314try: obj = urllib.request.urlopen(settings.get('PUBLIC_IP_URL', 'http://ip.42.pl/raw')) results = self.ip_regex.findall(obj.read()) if len(results) &gt; 0: # results[0] 获取IP地址即为110.90.122.1 self.my_ip = results[0] else: raise IOError("Could not get valid IP Address") obj.close() self.logger.debug("Current public ip: &#123;ip&#125;".format(ip=self.my_ip))except IOError: self.logger.error("Could not reach out to get public ip") pass建议修改代码，获取本机IP12self.my_ip = [(s.connect(('8.8.8.8', 53)), s.getsockname()[0], s.close()) for s in [socket.socket(socket.AF_INET, socket.SOCK_DGRAM)]][0][1]运行分布式爬虫在两个scrapy-cluster中运行相同Spider1execute(['scrapy', 'runspider', 'crawling/spiders/link_spider.py'])使用python kafka_monitor.py feed投递多个链接，使用DEBUG即可观察到链接分配情况使用SpiderKeeper管理分布式爬虫配置scrapyd管理集群第二个scrapy-cluster在第二台scrapy-cluster服务器上安装配置scrapyd，参考scrapyd 爬虫管理工具配置并修改配置123456[settings]default = crawling.settings[deploy]url = http://168.*.*.119:6800/project = crawling启动scrapyd后使用scrapyd-deploy工具部署两个scrapy-cluster上的爬虫项目。使用Spiderkeeper连接多个scrapy-cluster重新启动spiderkeeper，对接两个scrapy-cluster的管理工具scrapyd。1nohup spiderkeeper --server=http://168.*.*.118:6800 --server=http://168.*.*.119:6800 --username=admin --password=admin --database-url=sqlite:////root/spiderkeeper/SpiderKeeper.db &gt;&gt; /root/scrapy-cluster/spiderkeeper.log 2&gt;&amp;1 &amp;注意：要使用spiderkeeper管理同一个集群，爬虫项目名称必须一致，同时集群中scrapy-cluster配置相同spider任务浏览器访问http://168.*.*.118:5000 启动爬虫时即可看见两个scrapy-cluster集群配置，启动同名爬虫开始scrapy-cluster分布式爬虫启动分布式爬虫后状态]]></content>
  </entry>
</search>
