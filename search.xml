<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hexo+NexT v6.4.1 博客优化日志]]></title>
    <url>%2Fhexo-00-major.html</url>
    <content type="text"><![CDATA[记录 Hexo 优化及主题 NexT 美化@toc主题插件「盘古之白」為什麼你們就是不能加個空格呢？by pangu.js「盤古之白」，它劈開了全形字和半形字之間的混沌。另有研究顯示，打字的時候不喜歡在中文和英文之間加空格的人，感情路都走得很辛苦，有七成的比例會在 34 歲的時候跟自己不愛的人結婚，而其餘三成的人最後只能把遺產留給自己的貓。畢竟愛情跟書寫都需要適時地留白。與大家共勉之。1$ npm install pangu --savethemes/next/_config.yaml修改12345# Pangu Support# Dependencies: https://github.com/theme-next/theme-next-pangu# For more information: https://github.com/vinta/pangu.js# pangu: falsepangu: true]]></content>
      <tags>
        <tag>Hexo</tag>
        <tag>NexT</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache HBase ™ Reference Guide：Hbase 官方指南 3.0（一）「数据模型」]]></title>
    <url>%2Fhbase-001-data-model.html</url>
    <content type="text"><![CDATA[校验 Hbase 官方指南文档Apache HBase ™ Reference Guide Version 3.0.0-SNAPSHOTData Model 部分 No.20 到 No.32 小节@toc数据模型在 HBase 中，数据存储在具有行和列的表中。关系型数据库（RDBMS）中有相似的概念，这并不是有用的类比。不过，HBase 可以被认为是一个多维度的数据映射存储。HBase Data Model Terminology Hbase（数据模型术语）Table（表）HBase 表由多行组成。Row（列）HBase 中的一行由一个行键和一个或多个具有与之关联的值的列组成。行存储时，行按字母顺序排序。因此，行的 key 的设计就显得非常重要。数据的存储目标是相近的数据存储到一起。一个常用的行的 key 的格式是网站域名。如果你的行的 key 是域名，你应该将域名进行反转 (org.apache.www, org.apache.mail, org.apache.jira) 再存储。这样的话，所有 Apache 域名将会存储在一起，而不是基于子域名的首字母分散在各处。Column（列）HBase 中的列包含用「:」(冒号)分隔开的列族和列的限定符。Column Family（列族）因为性能的原因，列族物理上包含一组列和它们的值。每一个列族拥有一系列的存储属性，例如值是否缓存在内存中，数据是否要压缩或者他的行 key 是否要加密等等。表格中的每一行拥有相同的列族，尽管一个给定的行可能没有存储任何数据在一个给定的列族中。Column Qualifier（列限定符）列的限定符是列族中数据的索引。例如给定了一个列族 content，那么限定符可能是 content:html，也可以是 content:pdf。列族在创建表格时是确定的了，但是列的限定符是可变的，并且行之间可能有很大差异。Cell（单元）单元是由行、列族、列限定符、值和代表值版本的时间戳组成的。Timestamp（时间戳）时间戳是写在值旁边的一个用于区分值的版本的数据。默认情况下，时间戳表示的是当数据写入时 RegionSever 的时间点，但你也可以在写入数据时指定一个不同的时间戳。概念视图你可以读一下 Jim R 写的Understanding HBase and BigTable 博客来简单了解一下 HBase 的数据模型，另一个好的理解是 Amandeep Khurana. 的 Introduction to Basic Schema Design 。学习不同的方面的资料可能会帮助你更透彻地了解 HBase 的设计。所链接的文章覆盖本部分所讲的信息。接下来的例子是 取自 BigTable 中第二页中的例子，在此基础上做了些许的改变。一个名为 webable 的表格，表格中有两行（com.cnn.www 和 com.example.www）和三个列族（contents, anchor, 和 people）。在这个例子当中，第一行(com.cnn.www) 中 anchor 包含两列（anchor:cssnsi.com, anchor:my.look.ca）和 content 包含一列（contents:html）。这个例子中 com.cnn.www 拥有 5 个版本而 com.example.www 有一个版本。contents:html 列中包含给定网页的整个 HTML。anchor 限定符包含能够表示行的站点以及链接中文本。People 列族表示跟站点有关的人。列名按照所定义好的，一个列名的格式为列族名前缀加限定符。例如，列 contents:html 由列族 contents 和 html 限定符。冒号「:」用于将列族和列限定符分开。Table 6. Table webtable行键时间戳列族 contents列族 anchor列族 people“com.cnn.www”t9anchor:cnnsi.com = “CNN”“com.cnn.www”t8anchor:cnnsi.com = “CNN”“com.cnn.www”t6contents:html = “&lt;html&gt;…​”“com.cnn.www”t5contents:html = “&lt;html&gt;…​”“com.cnn.www”t3contents:html = “&lt;html&gt;…​”“com.example.www”t5contents:html = “&lt;html&gt;…​”people:author = “John Doe”在 HBase 中，表格中的单元如果是空将不占用空间或者事实上不存在。这就使得 HBase 看起来“稀疏”。表格视图不是唯一方式来查看 HBase 中数据，甚至不是最精确的。下面的方式以多维度映射的方式来表达相同的信息。这只是一个说明示例的模型可能不是严格准确的。1234567891011121314151617181920212223&#123; &quot;com.cnn.www&quot;: &#123; contents: &#123; t6: contents:html: &quot;&lt;html&gt;...&quot; t5: contents:html: &quot;&lt;html&gt;...&quot; t3: contents:html: &quot;&lt;html&gt;...&quot; &#125; anchor: &#123; t9: anchor:cnnsi.com = &quot;CNN&quot; t8: anchor:my.look.ca = &quot;CNN.com&quot; &#125; people: &#123;&#125; &#125; &quot;com.example.www&quot;: &#123; contents: &#123; t5: contents:html: &quot;&lt;html&gt;...&quot; &#125; anchor: &#123;&#125; people: &#123; t5: people:author: &quot;John Doe&quot; &#125; &#125;&#125;物理视图尽管在概念层次，表可能看起来是由一些列稀疏的行组成，但他们是通过列族来存储的。一个新建的限定符 (column_family:column_qualifier) 可以随时地添加到已存在的列族中。Table 7. 列族 anchor行键时间戳列族 anchor“com.cnn.www”t9anchor:cnnsi.com = “CNN”“com.cnn.www”t8anchor:my.look.ca = “CNN.com”Table 8. 列族 contents行键时间戳列族 contents“com.cnn.www”t6contents:html = “&lt;html&gt;…​”“com.cnn.www”t5contents:html = “&lt;html&gt;…​”“com.cnn.www”t3contents:html = “&lt;html&gt;…​”概念视图中的空单元实际上是没有进行存储的。因此对于返回时间戳为 t8 的 contents:html 的值的请求，结果为空。同样的，一个返回时间戳为 t9 的 anchor:my.look.ca 的值的请求，结果也为空。然而，如果没有指定时间戳的话，那么会返回特定列的最新值。对有多个版本的列，优先返回最新的值，因为时间戳是按照递减顺序存储的。因此对于一个返回 com.cnn.www 里面所有的列的值并且没有指定时间戳的请求，返回的结果会是时间戳为 t6 的 contents:html 的值、时间戳 t9 的 anchor:cnnsi.com 的值和时间戳 t8 的 anchor:my.look.ca 。关于 Apache Hbase 如何存储数据的内部细节，请查看 regions.arch.命名空间命名空间是一个类似于关系型数据库系统中的数据库的逻辑上的表分组的概念。这个抽象的概念为即将到来的多租户相关特性奠定了基础：配额管理 (HBASE-8410) - 限制命名空间可以使用的资源量（即区域，表）。命名空间安全管理 (HBASE-9206) - 为租户提供另一级别的安全管理。区域服务器组 (HBASE-6721) - 可以将命名空间 / 表固定到 RegionServers 的子集上，从而保证粗略的隔离级别。命名空间管理命名空间可以被创建、移除和修改。命名空间关系的指定是在创建表格通过指定一个完全限定表名的形式完成的：1&lt;table namespace&gt;:&lt;table qualifier&gt;Example 7. Examples1234567891011#Create a namespacecreate_namespace &apos;my_ns&apos;#create my_table in my_ns namespacecreate &apos;my_ns:my_table&apos;, &apos;fam&apos;#drop namespacedrop_namespace &apos;my_ns&apos;#alter namespacealter_namespace &apos;my_ns&apos;, &#123;METHOD =&gt; &apos;set&apos;, &apos;PROPERTY_NAME&apos; =&gt; &apos;PROPERTY_VALUE&apos;&#125;预定义命名空间有两种预定义的特殊的命名空间hbase – 系统命名空间, 用于包含 HBase 内部表default – 没有明确指定命名空间的表将会自动落入这个命名空间Example 8. Examples12345#namespace=foo and table qualifier=barcreate &apos;foo:bar&apos;, &apos;fam&apos;#namespace=default and table qualifier=barcreate &apos;bar&apos;, &apos;fam&apos;表在 Schema（模式）定义时预先声明表。行行键是未解释的字节。行是按照字典顺序进行排序的并且最小的排在前面。空的字节数据用来表示表格的命名空间的开头和结尾。列族列在 HBase 中是归入到列族里面的。一个列族的所有列成员都有相同的前缀。例如，列 courses:history 和 cources:math 是 cources 列族的成员，冒号用于将列族和列限定符分开。列族前缀必须由可打印输出的字符组成。列限定符可以由任意字节组成。列族必须在结构定义阶段预先声明好，而列则不需要在结构设计阶段预先定义，而是可以在表的创建和运行阶段动态生成。物理上来说，所有的列族成员都是存储在文件系统。因为调试和存储参数都是在列族级别完成，建议所有的列族都要拥有相同的访问模式和大小特征。单元一个 {row,column,version} 完全指定了 HBase 的一个单元。单元内容是未解释的字节数据模型操作数据模型的四个主要操作是 Get，Put，Scan 和 Delete。可以通过 Table 实例进行操作。GetGet 返回指定行的属性 Gets 通过 Table.get). 执行。PutPut 操作是在行键不存在时添加新行或者行键已经存在时进行更新。 Puts 是通过 Table.put) (写缓存) 或者 Table.batch) (没有写缓存) 执行的。ScansScan 允许为指定属性迭代多行。下面是表格实例中 Scan 的例子。假设一个表格里面有 “row1”, “row2”, “row3”，然后有另外一组行键为 “abc1”, “abc2”, 和 “abc3”。下面的例子展示如何设置一个 Scan 实例来返回以 “row” 开头的行。123456789101112131415161718192021222324252627282930313233public static final byte[] CF = "cf".getBytes(); public static final byte[] ATTR = "attr".getBytes(); ... Table table = ... // instantiate a Table instance Scan scan = new Scan(); scan.addColumn(CF, ATTR); scan.setRowPrefixFilter(Bytes.toBytes("row")); ResultScanner rs = table.getScanner(scan); try &#123; for (Result r = rs.next(); r != null; r = rs.next()) &#123; // process result... &#125; &#125; finally &#123; rs.close(); // always close the ResultScanner! &#125;需要说明的是通常最简单的指定 Scan 的一个特定停止点的方法是使用InclusiveStopFilter 类。DeleteDelete 操作是将一个行从表中移除. Deletes 通过 Table.delete)执行。HBase 不会立刻对数据的进行操作（可以理解为不对数据执行删除操作），而是为死亡数据创建一个称为墓碑的标签。这个墓碑和死亡数据会在重要精简工作中被删除。查看 version.delete 获取更多关于列的版本删除的信息，查看 compaction 获取关于压缩工作的更多信息。版本A {row, column, version} 在 HBase 完全指定一个单元。理论上来说行和列都一样的单元的数量是无限的，因为单元的地址是通过版本这个维度来区分的。行和列使用字节来表达，而版本是通过长整型来指定的。典型来说，这个长时间实例就像 java.util.Date.getTime() 或者 System.currentTimeMillis()返回的一样，以毫秒为单位，返回当前时间和 January 1, 1970 UTC 的时间差 。HBase 的版本维度以递减顺序存储，以致读取一个存储的文件时，返回的是最新版本的数据。关于单元的版本有许多的困扰，尤其是：如果多个数据写到一个具有相同版本的单元里，只能获取到最后写入的那个以非递增的版本顺序写入也是可以的。下面我们将描述 HBase 中版本维度是如何运作的。可以看 HBASE-2406 关于 HBase 版本的讨论。 Bending time in HBase 是关于 HBase 的版本或者时间维度的好读物。它提供了比这里更多的关于版本的细节信息。正如这里写到的，这里提到的覆盖存在的时间戳的限制将不再存在。这部分只是 Bruno Dumon 所写的关于版本的基本大纲。指定版本的存储数量版本的最大存储数量是列结构的一个部分并且在表格创建时指定，或者通过 alter 命令行，或者通过 HColumnDescriptor.DEFAULT_VERSIONS 来修改。HBase0.96 之前，默认数量是 3，HBase0.96 之后改为 1.Example 13. 修改一个列族的最大版本数这个例子使用 HBase Shell 来修改列族 f1 的最大版本数为 5，你也可以使用 HColumnDescriptor来实现。1hbase&gt; alter ‘t1′, NAME =&gt; ‘f1′, VERSIONS =&gt; 5Example 14. 修改一个列族的最小版本数 Modify你也可以通过指定最小半本书来存储列族。默认情况下，该值为零，意味着这个属性是禁用的。下面的例子是通过 HBase Shell 设置列族 f1 中的所有列的最小版本数为 2。你也可以通过 HColumnDescriptor来实现。1hbase&gt; alter ‘t1′, NAME =&gt; ‘f1′, MIN_VERSIONS =&gt; 2从 HBase0.98.2 开始，你可以通过设定在hbase-site.xml中设置 hbase.column.max.version 属性为所有新建的列指定一个全局的默认的最大版本数。版本和 HBase 操作在这部分我们来看一下版本维度在 HBase 的每个核心操作中的表现。Get/ScanGet 是通过获取 Scan 的第一个数据来实现的。下面的讨论适用于 Get 和 Scans.。默认情况下，如果你没有指定明确的版本，当你执行一个 Get 操作时，那个版本为最大值的单元将被返回（可能是也可能不是最新写人的那个）。默认的行为可以通过下面方式来修改：返回不止一个版本 查看 Get.setMaxVersions())返回最新版本以外的版本, 查看 Get.setTimeRange())想要获得小于或等于固定值的最新版本，仅仅通过使用一个 0 到期望版本的范围和设置最大版本数为 1 就可以实现获得一个特定时间点的最新版本的记录。默认的 Get 例子下面例子仅仅返回行的当前版本。1234567891011public static final byte[] CF = "cf".getBytes(); public static final byte[] ATTR = "attr".getBytes(); ... Get get = new Get(Bytes.toBytes("row1")); Result r = table.get(get); byte[] b = r.getValue(CF, ATTR); // returns current version of valueGet 版本的例下面是获得行的最新 3 个版本的例子：123456789101112131415public static final byte[] CF = "cf".getBytes(); public static final byte[] ATTR = "attr".getBytes(); ... Get get = new Get(Bytes.toBytes("row1")); get.setMaxVersions(3); // will return last 3 versions of row Result r = table.get(get); byte[] b = r.getValue(CF, ATTR); // returns current version of value List&lt;KeyValue&gt; kv = r.getColumn(CF, ATTR); // returns all versions of this columnPutPut 操作常常是以固定的时间戳来创建一个新单元。默认情况下，系统使用服务的 currentTimeMillis，但是你也可以为每一个列自己指定版本（长整型）。这就意味着你可以指定一个过去或者未来的时间点，或者不是时间格式的长整型。为了覆盖已经存在的值，对和那个你想要覆盖的单元完全一样的 row、column 和 version 进行 put 操作。隐式版本例子下面 Put 是以当前时间为版本的隐式操作1234567891011public static final byte[] CF = "cf".getBytes(); public static final byte[] ATTR = "attr".getBytes(); ... Put put = new Put(Bytes.toBytes(row)); put.add(CF, ATTR, Bytes.toBytes( data)); table.put(put);显示版本例子下面的 put 是显示指定时间戳的操作。12345678910111213public static final byte[] CF = "cf".getBytes(); public static final byte[] ATTR = "attr".getBytes(); ... Put put = new Put( Bytes.toBytes(row)); long explicitTimeInMs = 555; // just an example put.add(CF, ATTR, explicitTimeInMs, Bytes.toBytes(data)); table.put(put);警告: 版本时间戳是 HBase 内部用来计算数据的存活时间的。它最好避免自己设置。最好是将时间戳作为行的单独属性或者作为 key 的一部分，或者两者都有。DeleteThere are three different types of internal delete markers. See Lars Hofhansl’s blog for discussion of his attempt adding another, Scanning in HBase: Prefix Delete Marker.有三种不同的删除类型。可以看看 Lars Hofhansl 所写的博客 Scanning in HBase: Prefix Delete Marker.Delete: 列的指定版本Delete column: 列的所有版本Delete family: 特定列族里面的所有列。当要删除整个行时，HBase 将会在内部为每一个列族创建一个墓碑。删除通过创建一个墓碑标签来工作的。例如，让我们来设想我们要删除一个行。为此你可指定一个版本，或者使用默认的 currentTimeMillis 。这就是删除小于等于该版本的所有单元。HBase 不会修改数据，例如删除操作将不会立刻删除满足删除条件的文件。相反的，称为墓碑的会被写入，用来掩饰被删除的数据。当 HBase 执行一个压缩操作，墓碑将会执行一个真正地删除死亡值和墓碑自己的删除操作。如果你的删除操作指定的版本大于目前所有的版本，那么可以认为是删除整个行的数据。你可以在 Put w/timestamp → Deleteall → Put w/ timestamp fails 用户邮件列表中查看关于删除和版本之间的相互影响的有益信息。keyvalue 也可以到keyvalue 查看更多关于内部 KeyValue 格式的信息。删除标签会在下一次仓库压缩操作中被清理掉，除非为列族设置了 KEEP_DELETED_CELLS (查看 Keeping Deleted Cells)。为了保证删除时间的可配置性，你可以通过在 hbase-site.xml 。中 hbase.hstore.time.to.purge.deletes 属性来设置 TTL（生存时间）。如果 hbase.hstore.time.to.purge.deletes 没有设置或者设置为 0，所有的删除标签包括哪些墓碑都会在下一次精简操作中被干掉。此外，未来带有时间戳的删除标签将会保持到发生在 hbase.hstore.time.to.purge.deletes 加上代表标签的时间戳的时间和的下一次精简操作。This behavior represents a fix for an unexpected change that was introduced in HBase 0.94, and was fixed in HBASE-10118. The change has been backported to HBase 0.94 and newer branches.当前的局限性Deletes mask Puts 删除覆盖插入 / 更新删除操作覆盖插入 / 更新操作，即使 put 在 delete 之后执行的。可以查看 HBASE-2256. 还记得一个删除写入一个墓碑，只有当下一次精简操作发生时才会执行真正地删除操作。假设你执行了一个删除全部小于等于 T 的操作。在此之外又做了一个时间戳为 T 的 put 操作。这个 put 操作即使是发生在 delete 之后，也会被 delete 墓碑所覆盖。执行 put 的时候不会报错，不过当你执行一个 get 的时候会发现执行无效。你会在精简操作之后重新开始工作。如果你在 put 的使用的递增的版本，那么这些问题将不会出现。但如果你不在意时间，在执行 delelte 后立刻执行 put 的话，那么它们将有可能发生在同一时间点，这将会导致上述问题的出现。精简操作影响查询结果创建三个版本为 t1,t2,t3 的单元，并且设置最大版本数为 2. 所以当我们查询所有版本时，只会返回 t2 和 t3。但是当你删除版本 t2 和 t3 的时候，版本 t1 会重新出现。显然，一旦重要精简工作运行之后，这样的行为就不会再出现。（查看 Bendingtime in HBase.）排序次序HBase 中所有的数据模型操作返回的数据都是经过排序的。首先是行排序，其次是列族，接着是列限定符，最后是时间戳（递减排序，左右最新的记录最先返回）列元数据所有列的元数据都存储在一个列族的内部 KeyValue 实例中。因此，HBsase 不仅支持一行中有多列，而且支持行之间的列的差异多样化。跟踪列名是你的责任。唯一获取一个列族的所有列的方法是处理所有的行。查看 keyvalue获得更多关于 HBase 内部如何存储数据的信息。JoinsHBase 是否支持 join 是一个常见的问题，答案是没有，至少没办法像 RDBMS 那样支持（例如等价式 join 或者外部 join）。正如本章节所阐述的，HBase 中读取数据的操作是 Get 和 Scan。然而，这不意味着等价式 join 功能没办法在你的应用中实现，但是你必须自己实现。两种主要策略是将数据非结构化地写到 HBase 中，或者查找表格然后在应用中或者 MapReduce 代码中实现 join 操作（正如 RDBMS 所演示的，将根据表格的大小会有几种不同的策略，例如嵌套使循环和 hash-join）。哪个是最好的方法？这将依赖于你想做什么，没有一种方案能够应对各种情况。ACID查看 ACID Semantics. Lars Hofhansl 也写了一份报告 ACID in HBase.]]></content>
      <tags>
        <tag>Hbase</tag>
        <tag>翻译</tag>
        <tag>Guide</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软考（二）：操作系统]]></title>
    <url>%2Fsoftexam-002-os.html</url>
    <content type="text"><![CDATA[记录操作系统中知识点@toc考点进程进程的基本概念以及状态变化进程死锁进程同步、复制，信号量，前趋图，PV 原语存储其他细节进程就绪状态：进程已得到运行所需资源，只等待 CPU 的调度便可运行；运行状态：进程已得到运行所需资源，并且得到了 CPU 的调度；等待状态：不具备运行条件、等待时机的状态。另：等待状态也称阻塞状态；挂起：把该进程从内存中搬到外存上。激活：又叫唤醒或恢复，操作是一样的，只是叫法不一样而已，该操作是把外存上的某个进程弄到内存上。为什么要引入挂起和激活操作呢？用户的需要。用户调试一个程序的时候，运行该程序一多半了，但是，忽然发现该程序此时有 Bug，用户想停下来修改，但是修改后，用户又不想从头开始运行该程序，此为一因。操作系统的需要。操作系统管理着资源的分配，它无法忍受那些占着资源而不运行的程序，另外，这些进程也会妨碍系统的运行速度，此为一因。等等。进程的死锁进程管理师操作系统的核心，但如果设计不当，就会出现死锁的问题。如果一个进程在等待一个不可能发生的事，则进程就死锁了。而如果一个或多个进程产生死锁，就会造成系统死锁例如：系统有 3 个进程：A、B、C。这三个进程都需要 5 个系统资源。如果系统有 13 个资源，则不可能发生死锁。死锁发生的必要条件互斥条件：即一个资源每次只能被一个进程使用，在操作系统中这是真实存在的情况。保持和等待条件：有一个进程已经获得了一些资源，但因请求其他资源被阻塞时，对已获取的资源保持不放。不剥夺条件：有些系统不可资源是不可剥夺的，当某个进程已获得这种资源后，系统不能强行收回，只能由进程使用完时自己释放。环路等待条件：若干个进程形成环形链，每个都占用对方要申请的下一个资源。解决死锁的策略死锁预防：例如：要求用户申请资源时一起申请所需的全部资源，这就破坏了保持和等待条件；将资源分层，得到上一层资源后，才能够申请下一层资源，它破坏了环路等待条件。预防通常会降低系统的效率。死锁避免：避免是指进程在每次申请资源时判断这些操作是否安全，典型算法是「银行家算法」。但这种算法会增加系统的开销。死锁检测：死锁解除银行家算法著名的银行家算法，最早是由 Dijkstra 提出来的。它是一种最有代表性的避免死锁的算法。在避免死锁方法中允许进程动态地申请资源，但系资源分配之前，应先计算此次分配资源的安全性，若分配不会导致系统进入不安全状态，则分配，否则等待。银行家算法最重要的就是判断是可用资源和仍需资源之间的关系，如果可用资源数大于人需资源数，那么我们认为这个进程就是可以执行的，也是安全的，反之，便是不安全的。所以重中之重的是找到各种资源数。对进程的判断遵循以下步骤:计算系统开始时所有的资源数, 即开始的可用资源数;在仍需资源数和可用资源数中作比较, 找到符合条件的进程, 最后修改进程执行完毕时系统的可用资源数;继续比较剩余进程和可用资源数, 找到下边可以执行的进程;依次类推;【例】假设系统中有 3 类互斥资源 R1、R2、R3，可用资源分别是 9、8、5,。在 T0 时刻系统中有 P1、P2、P3、P4 和 P5 五个进程，这些进程对资源的最大需求量和已分配资源数如下表所示，则进程如何执行是安全的。这里需要强调的是，无论题目中给出何种条件，我们只要找到以下信息便可从容应对各种变化：【注】：可用资源：表示相应的进程执行完毕（即释放该进程占用的资源）以后可用的资源，满足公式可用资源 = 可用资源 + 已分配资源，（因为已分配的资源将会在进程执行完毕以后释放，所以可用资源会不断增多，进程执行完毕便会全部释放）同时它也是下一个进程执行时可用的资源。** 需要说明的是根据进程执行情况的不同，每次填入表格中的可用资源也不会相同（因为每个进程分配的资源是有差异的），那么执行顺序也会有所差异，合理即可。仍需资源：仍需资源数 = 最大需求量 - 已分配资源数，据此公式可以求得 R1、R2、R3 在不同的进程时仍需的资源数，如上表中所示。按照之前所讲的步骤，实现如下：1234567R1 已分配的总资源数为 1+2+2+1+1=7R2 已分配的总资源数为 2+1+1+2+1=7R3 已分配的总资源数为 1+1+0+0+3=5 则 R1 R2 R3 可用资源数分别为 R1=9-7=2R2=8-7=1R3=5-5=0开始有的资源数 R1 R2R3 分别为 2、1、0, 所以从仍需资源中查找 (需要说明的是查找的时候以最少资源数作为限定条件能够较快地找出结果), 只有 P2 进程符合条件, 此时可用资源变为 4、2、1；接下来在在其余的进程中查找符合条件的进程, 只能执行 P4, 此时可用资源变为 5、4、1, 以此类推, 按照以上的步骤即可找到所有进程执行的顺序 P2-&gt;P4-&gt;P5-&gt;P1-&gt;P3；以上便是有关银行家算法的计算过程。前趋图前趋图（Precedence Graph）是一个有向无循环图，记为 DAG（Directed Acyclic Graph），用于描述进程之间执行的前后关系。途中的每个节点可用于描述一个程序段或进城，乃至一条语句；节点间的有向边则用于表示两个节点之间存在的偏序（Partial Order）或前驱关系（Precedence Relation）「→」如果]]></content>
      <tags>
        <tag>软考</tag>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软考（一）：文法]]></title>
    <url>%2Fsoftexam-001-grammar.html</url>
    <content type="text"><![CDATA[记录编译原理中文法的知识点编译原理顾名思义就是处理高级语言，使之称为计算机能够识别的语言（低级语言）的原理。而文法呢？就是用来描述程序设计语言的方法。类似佛法, 用来描述佛家的诵经禅道的规则的。@toc概念正如英语是由句子组成的集合，而句子又是由单词和标点符号组成的序列那样。程序设计语言 C 语言，是由 C 程序所组成的集合，而程序是由类似 if , begin, end 的符号，字母和数字这样一些基本符号所组成。从字面上看，每个程序都是一个 “基本符号” 串，设有一基本符号串，那么 C 语言可看成是在这个基本符号集上定义的，按一定规则构成的一切基本符号串组成的集合。通俗的讲就是：根据一些指定的规则，来确定编程语言的语法，从而实现编译器的功能。终结符和非终结符文法是由非终结符（大写字母）和终结符（小写字母）以及 “—&gt;” 组成的。1234A—&gt; aB—&gt;dbaS—&gt; AbadB—&gt;d通过上面的几个例子可以看出：非终结符 A、B、S，一般是写在左边，而终结符 a、dba、b，一般是写在右边的。当然习惯的写法是非终结符用 S（Start）表示，写在左边，自然而然的小写的就在右边了，这样也便于我们记忆文法的表示方式。1234567G2[S]S-&gt;ApS-&gt;BqA-&gt;aA-&gt;cAB-&gt;bb-&gt;dB如上所示：在这个推导式的集合中，存在六个推导式。其中 S、A、B 为非终结符。a、b、c、d、q、p 为终结符。终结符是原子不可分的。分类在 1956 年的春天，一个叫乔姆斯基 (Chomsky) 的人发明了上述文法，他觉得有些文法存在着相似的形式，于是他就给文法分了一下类。首先有一个前提：设有一个组合 G=(Vn,Vt,P,S)。其中 Vn 是非终结符的集合，Vt 是终结符的集合，P 是推导式的一个集合，S 是开始符。就上面的例子来说，A、B、S 是 Vn，a、dba、b 是 Vt，整个的集合为 P。0 型文法这是最简单的一个文法。它比较宽容，没有那么多的限制条件。左边必须要包含这些元素或者元素组合中的至少一个非终结符，右边可以是这些元素的任意组合，这样就构成了 0 型文法。由于限制最少，所以见到的文法至少是一个 0 型文法。如：1A-&gt;a1 型文法（上下文有关文法）1 型文法也叫上下文有关文法，此文法对应于线性有界自动机。它在 0 型文法的基础之上，只添加了一个要求：右边的长度&gt;= 左边的长度（终结符或非终结符的个数）。A—&gt; a、B—&gt;dba 则是 1 型文法，而 adB—&gt;d 不符合 1 型文法要求 注意这里有一个特殊的形式 S—&gt; ∑（∑表示空），也是一个 1 型文法。2 型文法（上下文无关文法）2 型文法也叫上下文有关文法，此文法对应于下推自动机自动机。它在 1 型文法的基础上，有增加了一个要求：左边必须是非终结符（个数不限）。 如：AB—&gt;de 属于 2 型文法，而 Aa—&gt;DE 则不是，因为 Aa 中含有 a。3 型文法（正规文法）3 型文法也叫正规文法，它对应有限状态自动机。它是在 2 型的基础上提出了要么一个非终结符推出一个终结符，要么一个非终结符推出一个终结符并且带一个非终结符。A-&gt;a | aB（右线性）或 A-&gt;a | Ba（左线性）而上面的左线性、右线性是相互独立的。如：A—&gt;b、A—&gt;bD 这是 3 型文法但是 A—&gt;b、A—&gt;bD、A—&gt;Db 则不是 3 型文法。从这里可以看出，对于 3 型文法，它不是左右线性的“组合”，要么都是右线性，要么都是左线性。三种文法关系3\text{型文法} \subset2\text{型文法} \subset1\text{型文法} \subset0\text{型文法}正规式item文法产生式正规式规则 1A->xB,B->yA=xy规则 2A->xA\vert yA=x^*y规则 3A->x,A->yA=x\vert y实例实例 1A->\varepsilon|aB,B->Ab|a判断上面推导式中满足什么类型的文法首先要判断哪些是 终结符和非终结符 , 简单来讲终结符就是终结的, 最小的不可拆分的元素, 而不是终结符的都是非终结符. 这个应该是没有任何问题的, 所以上题中, 的非终结符就是 AB, 其他都是非终结符. 而 0 型文法中, 讲到只需要在 p 中至少有一个非终结符, 也就是在推导式的左边至少存在一个非终结符就可以了. 这样一来, 我们看到在等式的左边, 两个都是非终结符. 肯定满足 0 型文法, 下面就是 1 型文法了, 1 型文法是怎么规定的呢? 在 0 型文法的基础上, 推导式的 左边的长度肯定小于或者等于右边的长度 , 题中 p 集合里面左边的长度都小于右边的长度, 所以肯定符合 1 型文法; 接下来就是看看是否满足 2 型文法了. 2 型文法是怎么来限定的呢？在 1 型文法的基础上，在推导式的 左边每个都是非终结符 ，如题，每个推导式的左边都是非终结符，所以肯定是 2 型文法了；3 型文法的意思就是 2 型文法的基础上，看看是否满足右线性或者左线性，我们将这个推导式分开来判断。A—&gt;a 或者 A—-&gt;aB，第一个拆开的推导式是右线性，而 B—-&gt;A 是左线性的，3 型文法是怎么规定的呢？是符合 左线性或者右线性 。实例 2\begin{align*} &\mbox{文法}: G: S->xSx | y\mbox{所识别的语言是}（）。\\ &A. xyx\\ &B. (xyz)^*\\ &C. x^*yx^*\\ &D. x^nyx^n(n\geq0)\\ \end{align*}解析：D。S->xSx->xxSxx->x…S…x->x…y...x->x^nyx^n因为 S-&gt;y，所以 n 可以为 0，即 n 的范围为大于等于 0。实例 312345给定文法 A-&gt;bA|ca, 该文法的句子是：A. bbaB. cabC. bcaD. cba解析：C。A-&gt;bA-&gt;bca。第二次替换 A 的时候，使用候选式 A-&gt;ca 即可。实例 4\begin{align*} &\text{对于一下编号为①，②，③的正则式，正确的说法是___。}\\ &\begin{matrix} \text{①}(aa^*|ab)^b & \text{②}(a|b)^b & \text{③}((a|b)^*|aa)^b \\ \qquad \\ \end{matrix}\\ &\begin{matrix} A. \text{正则式①，②等价} & \qquad & B. \text{正则式①，③等价}\\ C. \text{正则式②，③等价} & \qquad & D. \text{正则式①，②，③互不等价} \end{matrix} \end{align*}解析：C。①中任意个前始终有 b，②中为任意个 a 或 b，故①②不等价，③中包含②并且任意长度 a 和故 b 组成的串，aa 限制条件不是必要条件故②③等价。实例 5\begin{align*} &\text{语言}L=\{a^mb^n|m\geq0,n\geq1\} \text{的正规表达式是_____。}\\ &\begin{matrix} A. a^*bb^* & B. aa^*bb^* & C. aa^*b^* & D. a^*b^* \end{matrix} \end{align*}解析：A。$a^$ 代表若干个 a 包括零个 a，$b^b$ 表示至少一个 b。default]]></content>
      <tags>
        <tag>软考</tag>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world.html</url>
    <content type="text"><![CDATA[github + hexo + NexTWelcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.Quick StartCreate a new post1$ hexo new "My New Post"More info: WritingRun server1$ hexo serverMore info: ServerGenerate static files1$ hexo generateMore info: GeneratingDeploy to remote sites1$ hexo deployMore info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Scrapy-cluster 分布式爬虫]]></title>
    <url>%2Fpython-001-scrapy-cluster.html</url>
    <content type="text"><![CDATA[记录搭建 scrapy-cluster 以及管理工具 scrapyd+spiderkeeper基于 Scrapy-cluster 库的 kafka-monitor 可以实现分布式爬虫Scrapyd+Spiderkeeper 实现爬虫的可视化管理环境IPRole168.*.*.118Scrapy-cluster,scrapyd,spiderkeeper168.*.*.119Scrapy-cluster,scrapyd,kafka,redis,zookeeper12345678# cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core) # python -VPython 2.7.5# java -versionopenjdk version &quot;1.8.0_181&quot;OpenJDK Runtime Environment (build 1.8.0_181-b13)OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)Zookeeper 单机配置下载并配置1234567# wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz# tar -zxvf zookeeper-3.4.13.tar.gz# cd zookeeper-3.4.13/conf# cp zoo_sample.cfg zoo.cfg# cd ..# PATH=/opt/zookeeper-3.4.13/bin:$PATH# echo &apos;export PATH=/opt/zookeeper-3.4.13/bin:$PATH&apos; &gt; /etc/profile.d/zoo.sh单节点启动123456# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/zookeeper-3.4.13/bin/../conf/zoo.cfgError contacting service. It is probably not running.# zkServer.sh startkafka 单机配置下载123# wget http://mirrors.hust.edu.cn/apache/kafka/2.0.0/kafka_2.12-2.0.0.tgz# tar -zxvf kafka_2.12-2.0.0.tgz# cd kafka_2.12-2.0.0/配置123456789101112# vim config/server.properties############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.broker.id=0 # kafka 的机器编号，host.name = 168.*.*.119 # 绑定 ipport=9092 # 默认端口 9092，# Switch to enable topic deletion or not, default value is falsedelete.topic.enable=true############################# Zookeeper #############################zookeeper.connect=localhost:2181启动1nohup bin/kafka-server-start.sh config/server.properties &amp;停止命令 bin/kafka-server-stop.sh config/server.propertiesredis 单机配置安装配置123# yum -y install redis# vim /etc/redis.confbind 168.*.*.119启动1# systemctl start redis.servicescrapy-cluster 单机配置123# git clone https://github.com/istresearch/scrapy-cluster.git# cd scrapy-cluster# pip install -r requirements.txt离线运行单元测试, 以确保一切似乎正常1# ./run_offline_tests.sh修改配置123# vim kafka-monitor/settings.py# vim redis-monitor/settings.py# vim crawlers/crawling/settings.py修改以下12345678910111213141516# Redis host configurationREDIS_HOST = &apos;168.*.*.119&apos;REDIS_PORT = 6379REDIS_DB = 0KAFKA_HOSTS = &apos;168.*.*.119:9092&apos;KAFKA_TOPIC_PREFIX = &apos;demo&apos;KAFKA_CONN_TIMEOUT = 5KAFKA_APPID_TOPICS = FalseKAFKA_PRODUCER_BATCH_LINGER_MS = 25 # 25 ms before flushKAFKA_PRODUCER_BUFFER_BYTES = 4 * 1024 * 1024 # 4MB before blocking# Zookeeper SettingsZOOKEEPER_ASSIGN_PATH = &apos;/scrapy-cluster/crawler/&apos;ZOOKEEPER_ID = &apos;all&apos;ZOOKEEPER_HOSTS = &apos;168.*.*.119:2181&apos;启动监听12# nohup python kafka_monitor.py run &gt;&gt; /root/scrapy-cluster/kafka-monitor/kafka_monitor.log 2&gt;&amp;1 &amp;# nohup python redis_monitor.py &gt;&gt; /root/scrapy-cluster/redis-monitor/redis_monitor.log 2&gt;&amp;1 &amp;scrapyd 爬虫管理工具配置安装1# pip install scrapyd配置12# sudo mkdir /etc/scrapyd# sudo vi /etc/scrapyd/scrapyd.conf1234567891011121314151617181920212223242526272829[scrapyd]eggs_dir = eggslogs_dir = logsitems_dir =jobs_to_keep = 5dbs_dir = dbsmax_proc = 0max_proc_per_cpu = 10finished_to_keep = 100poll_interval = 5.0bind_address = 0.0.0.0http_port = 6800debug = offrunner = scrapyd.runnerapplication = scrapyd.app.applicationlauncher = scrapyd.launcher.Launcherwebroot = scrapyd.website.Root[services]schedule.json = scrapyd.webservice.Schedulecancel.json = scrapyd.webservice.Canceladdversion.json = scrapyd.webservice.AddVersionlistprojects.json = scrapyd.webservice.ListProjectslistversions.json = scrapyd.webservice.ListVersionslistspiders.json = scrapyd.webservice.ListSpidersdelproject.json = scrapyd.webservice.DeleteProjectdelversion.json = scrapyd.webservice.DeleteVersionlistjobs.json = scrapyd.webservice.ListJobsdaemonstatus.json = scrapyd.webservice.DaemonStatus启动1# nohup scrapyd &gt;&gt; /root/scrapy-cluster/scrapyd.log 2&gt;&amp;1 &amp;建议做 Nginx 反向代理启动异常1234567891011121314File &quot;/usr/local/lib/python3.6/site-packages/scrapyd-1.2.0-py3.6.egg/scrapyd/app.py&quot;, line 2, in &lt;module&gt;from twisted.application.internet import TimerService, TCPServerFile &quot;/usr/local/lib64/python3.6/site-packages/twisted/application/internet.py&quot;, line 54, in &lt;module&gt;from automat import MethodicalMachineFile &quot;/usr/local/lib/python3.6/site-packages/automat/__init__.py&quot;, line 2, in &lt;module&gt;from ._methodical import MethodicalMachineFile &quot;/usr/local/lib/python3.6/site-packages/automat/_methodical.py&quot;, line 210, in &lt;module&gt; class MethodicalInput(object):File &quot;/usr/local/lib/python3.6/site-packages/automat/_methodical.py&quot;, line 220, in MethodicalInput @argSpec.defaultbuiltins.TypeError: &apos;_Nothing&apos; object is not callableFailed to load application: &apos;_Nothing&apos; object is not callable解决：Automat 降级1pip install Automat==0.6.0Spiderkeeper 爬虫管理界面配置安装1pip install SpiderKeeper启动12mkdir /root/spiderkeeper/nohup spiderkeeper --server=http://168.*.*.118:6800 --username=admin --password=admin --database-url=sqlite:////root/spiderkeeper/SpiderKeeper.db &gt;&gt; /root/scrapy-cluster/spiderkeeper.log 2&gt;&amp;1 &amp;浏览器访问 http://168.*.*.118:5000使用 Spiderkeeper 管理爬虫使用 scrapyd-deploy 部署爬虫项目修改 scrapy.cfg 配置1vim /root/scrapy-cluster/crawler/scrapy.cfg123456[settings]default = crawling.settings[deploy]url = http://168.*.*.118:6800/project = crawling添加新的 spider1cd /root/scrapy-cluster/crawler/crawling/spider使用 scrapyd-deploy 部署项目123456# cd /root/scrapy-cluster/crawler# scrapyd-deploy Packing version 1536225989Deploying to project &quot;crawling&quot; in http://168.*.*.118:6800/addversion.jsonServer response (200):&#123;&quot;status&quot;: &quot;ok&quot;, &quot;project&quot;: &quot;crawling&quot;, &quot;version&quot;: &quot;1536225989&quot;, &quot;spiders&quot;: 3, &quot;node_name&quot;: &quot;ambari&quot;&#125;spiderkeeper 配置爬虫项目登录 Spiderkeeper 创建项目使用 scrapy.cfg 中配置的项目名创建后再 Spiders-&gt;Dashboard 中看到所有 spiderScrapy-cluster 分布式爬虫Scrapy Cluster 需要在不同的爬虫服务器之间进行协调，以确保最大的内容吞吐量，同时控制集群服务器爬取网站的速度。Scrapy Cluster 提供了两种主要策略来控制爬虫对不同域名的攻击速度。这由爬虫的类型与 IP 地址确定，但他们都作用于不同的域名队列。Scrapy-cluster 分布式爬虫，分发网址是基于 IP 地址。在不同的机器上启动集群，不同服务器上的每个爬虫去除队列中的所有链接。部署集群中第二个 scrapy-cluster配置一台新的服务器参照scrapy-cluster 单机配置, 同时使用第一台服务器配置 kafka-monitor/settings.py redis-monitor/settings.py crawling/settings.pyCurrent public ip 问题由于两台服务器同时部署在相同内网，spider 运行后即获取相同 Current public ip，导致 scrapy-cluster 调度器无法根据 IP 分发链接12018-09-07 16:08:29,684 [sc-crawler] DEBUG: Current public ip: b&apos;110.90.122.1&apos;参考代码 /root/scrapy-cluster/crawler/crawling/distributed_scheduler.py 第 282 行：1234567891011121314try: obj = urllib.request.urlopen(settings.get('PUBLIC_IP_URL', 'http://ip.42.pl/raw')) results = self.ip_regex.findall(obj.read()) if len(results) &gt; 0: # results[0] 获取 IP 地址即为 110.90.122.1 self.my_ip = results[0] else: raise IOError("Could not get valid IP Address") obj.close() self.logger.debug("Current public ip: &#123;ip&#125;".format(ip=self.my_ip))except IOError: self.logger.error("Could not reach out to get public ip") pass建议修改代码，获取本机 IP12self.my_ip = [(s.connect(('8.8.8.8', 53)), s.getsockname()[0], s.close()) for s in [socket.socket(socket.AF_INET, socket.SOCK_DGRAM)]][0][1]运行分布式爬虫在两个 scrapy-cluster 中运行相同 Spider1execute(['scrapy', 'runspider', 'crawling/spiders/link_spider.py'])使用 python kafka_monitor.py feed 投递多个链接，使用 DEBUG 即可观察到链接分配情况使用 SpiderKeeper 管理分布式爬虫配置 scrapyd 管理集群第二个 scrapy-cluster在第二台 scrapy-cluster 服务器上安装配置 scrapyd，参考 scrapyd 爬虫管理工具配置 并修改配置123456[settings]default = crawling.settings[deploy]url = http://168.*.*.119:6800/project = crawling启动 scrapyd 后使用 scrapyd-deploy 工具部署两个 scrapy-cluster 上的爬虫项目。使用 Spiderkeeper 连接多个 scrapy-cluster重新启动 spiderkeeper，对接两个 scrapy-cluster 的管理工具 scrapyd。1nohup spiderkeeper --server=http://168.*.*.118:6800 --server=http://168.*.*.119:6800 --username=admin --password=admin --database-url=sqlite:////root/spiderkeeper/SpiderKeeper.db &gt;&gt; /root/scrapy-cluster/spiderkeeper.log 2&gt;&amp;1 &amp;注意：要使用 spiderkeeper 管理同一个集群，爬虫项目名称必须一致，同时集群中 scrapy-cluster 配置相同 spider 任务浏览器访问 http://168.*.*.118:5000 启动爬虫时即可看见两个 scrapy-cluster 集群配置，启动同名爬虫开始 scrapy-cluster 分布式爬虫启动分布式爬虫后状态]]></content>
  </entry>
</search>
