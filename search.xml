<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[软考（一）：文法]]></title>
    <url>%2Fsoftexam-001-grammar.html</url>
    <content type="text"><![CDATA[记录编译原理中文法的知识点编译原理顾名思义就是处理高级语言，使之称为计算机能够识别的语言（低级语言）的原理。而文法呢？就是用来描述程序设计语言的方法。类似佛法,用来描述佛家的诵经禅道的规则的。 @toc 概念正如英语是由句子组成的集合，而句子又是由单词和标点符号组成的序列那样。程序设计语言 C 语言，是由 C 程序所组成的集合，而程序是由类似 if , begin, end 的符号，字母和数字这样一些基本符号所组成。 从字面上看，每个程序都是一个“基本符号”串，设有一基本符号串，那么 C 语言可看成是在这个基本符号集上定义的，按一定规则构成的一切基本符号串组成的集合。 通俗的讲就是：根据一些指定的规则，来确定编程语言的语法，从而实现编译器的功能。 终结符和非终结符文法是由非终结符（大写字母）和终结符（小写字母）以及“—&gt;”组成的。 1234A—&gt; aB—&gt;dbaS—&gt; AbadB—&gt;d 通过上面的几个例子可以看出：非终结符A、B、S，一般是写在左边，而终结符a、dba、b，一般是写在右边的。当然习惯的写法是非终结符用S（Start）表示，写在左边，自然而然的小写的就在右边了，这样也便于我们记忆文法的表示方式。 1234567G2[S]S-&gt;ApS-&gt;BqA-&gt;aA-&gt;cAB-&gt;bb-&gt;dB 如上所示：在这个推导式的集合中，存在六个推导式。其中S、A、B为非终结符。a、b、c、d、q、p为终结符。终结符是原子不可分的。 分类在1956年的春天，一个叫乔姆斯基(Chomsky)的人发明了上述文法，他觉得有些文法存在着相似的形式，于是他就给文法分了一下类。 首先有一个前提：设有一个组合G=(Vn,Vt,P,S)。其中Vn是非终结符的集合，Vt是终结符的集合，P是推导式的一个集合，S是开始符。就上面的例子来说，A、B、S是Vn，a、dba、b是Vt，整个的集合为P。 0型文法这是最简单的一个文法。它比较宽容，没有那么多的限制条件。左边必须要包含这些元素或者元素组合中的至少一个非终结符，右边可以是这些元素的任意组合，这样就构成了0型文法。由于限制最少，所以见到的文法至少是一个0型文法。如：1A-&gt;a 1型文法（上下文有关文法）1型文法也叫上下文有关文法，此文法对应于线性有界自动机。 它在0型文法的基础之上，只添加了一个要求：右边的长度&gt;=左边的长度（终结符或非终结符的个数）。A—&gt; a、B—&gt;dba则是1型文法，而adB—&gt;d不符合1型文法要求 注意这里有一个特殊的形式 S—&gt; ∑（∑表示空），也是一个1型文法。 2型文法（上下文无关文法）2型文法也叫上下文有关文法，此文法对应于下推自动机自动机。 它在1型文法的基础上，有增加了一个要求：左边必须是非终结符（个数不限）。 如：AB—&gt;de 属于2型文法，而 Aa—&gt;DE则不是，因为Aa中含有a。 3型文法（正规文法）3型文法也叫正规文法，它对应有限状态自动机。 它是在2型的基础上提出了要么一个非终结符推出一个终结符，要么一个非终结符推出一个终结符并且带一个非终结符。 A-&gt;a | aB（右线性）或 A-&gt;a | Ba（左线性） 而上面的左线性、右线性是相互独立的。如：A—&gt;b、A—&gt;bD这是3型文法 但是A—&gt;b、A—&gt;bD、A—&gt;Db则不是3型文法。从这里可以看出，对于3型文法，它不是左右线性的“组合”，要么都是右线性，要么都是左线性。 三种文法关系 3{型文法}\subset2{型文法}\subset1{型文法}\subset0{型文法} 型 文 法 型 文 法 型 文 法 型 文 法 正规式 item 文法产生式 正规式 规则1 A-&gt;xB,B-&gt;y A=xy 规则2 A-&gt;xA\vert y A=x^*y 规则3 A-&gt;x,A-&gt;y A=x\vert y 实例实例1 A-&gt;\varepsilon|aB,B-&gt;Ab|a 判断上面推导式中满足什么类型的文法 首先要判断哪些是==终结符和非终结符==,简单来讲终结符就是终结的,最小的不可拆分的元素,而不是终结符的都是非终结符.这个应该是没有任何问题的,所以上题中,的非终结符就是AB,其他都是非终结符.而0型文法中,讲到只需要在p中至少有一个非终结符,也就是在推导式的左边至少存在一个非终结符就可以了.这样一来,我们看到在等式的左边,两个都是非终结符.肯定满足0型文法,下面就是1型文法了,1型文法是怎么规定的呢?在0型文法的基础上,推导式的==左边的长度肯定小于或者等于右边的长度==,题中p集合里面左边的长度都小于右边的长度,所以肯定符合1型文法;接下来就是看看是否满足2型文法了.2型文法是怎么来限定的呢？在1型文法的基础上，在推导式的==左边每个都是非终结符==，如题，每个推导式的左边都是非终结符，所以肯定是2型文法了；3型文法的意思就是2型文法的基础上，看看是否满足右线性或者左线性，我们将这个推导式分开来判断。A–&gt;a或者A—&gt;aB，第一个拆开的推导式是右线性，而B—&gt;A是左线性的，3型文法是怎么规定的呢？是符合==左线性或者右线性==。 实例2$$\begin{align}&amp;{文法}: G: S-&gt;xSx | y{所识别的语言是}（）。\&amp;A. xyx\&amp;B. (xyz)^\&amp;C. x^yx^\&amp;D. x^nyx^n(n\geq0)\\end{align*}$$ 解析：D。 S-&gt;xSx-&gt;xxSxx-&gt;x…S…x-&gt;x…y…x-&gt;x^nyx^n 因为S-&gt;y，所以 n 可以为0，即 n 的范围为大于等于0。 实例312345给定文法A-&gt;bA|ca,该文法的句子是：A. bbaB. cabC. bcaD. cba 解析：C。A-&gt;bA-&gt;bca。第二次替换A的时候，使用候选式A-&gt;ca 即可。 实例4 对于\ 对 于]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world.html</url>
    <content type="text"><![CDATA[github + hexo + NexT Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Scrapy-cluster分布式爬虫]]></title>
    <url>%2Fpython-001-scrapy-cluster.html</url>
    <content type="text"><![CDATA[记录搭建scrapy-cluster以及管理工具scrapyd+spiderkeeper 基于Scrapy-cluster库的kafka-monitor可以实现分布式爬虫 Scrapyd+Spiderkeeper实现爬虫的可视化管理 环境 IP Role 168.*.*.118 Scrapy-cluster,scrapyd,spiderkeeper 168.*.*.119 Scrapy-cluster,scrapyd,kafka,redis,zookeeper 12345678# cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core) # python -VPython 2.7.5# java -versionopenjdk version "1.8.0_181"OpenJDK Runtime Environment (build 1.8.0_181-b13)OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode) Zookeeper 单机配置 下载并配置 1234567# wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz# tar -zxvf zookeeper-3.4.13.tar.gz# cd zookeeper-3.4.13/conf# cp zoo_sample.cfg zoo.cfg# cd ..# PATH=/opt/zookeeper-3.4.13/bin:$PATH# echo 'export PATH=/opt/zookeeper-3.4.13/bin:$PATH' &gt; /etc/profile.d/zoo.sh 单节点启动 123456# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/zookeeper-3.4.13/bin/../conf/zoo.cfgError contacting service. It is probably not running.# zkServer.sh start kafka 单机配置 下载 123# wget http://mirrors.hust.edu.cn/apache/kafka/2.0.0/kafka_2.12-2.0.0.tgz# tar -zxvf kafka_2.12-2.0.0.tgz# cd kafka_2.12-2.0.0/ 配置 123456789101112# vim config/server.properties############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.broker.id=0 # kafka的机器编号，host.name = 168.*.*.119 # 绑定ipport=9092 # 默认端口9092，# Switch to enable topic deletion or not, default value is falsedelete.topic.enable=true############################# Zookeeper #############################zookeeper.connect=localhost:2181 启动 1nohup bin/kafka-server-start.sh config/server.properties &amp; 停止命令bin/kafka-server-stop.sh config/server.properties redis 单机配置 安装配置 123# yum -y install redis# vim /etc/redis.confbind 168.*.*.119 启动 1# systemctl start redis.service scrapy-cluster 单机配置123# git clone https://github.com/istresearch/scrapy-cluster.git# cd scrapy-cluster# pip install -r requirements.txt 离线运行单元测试,以确保一切似乎正常 1# ./run_offline_tests.sh 修改配置 123# vim kafka-monitor/settings.py# vim redis-monitor/settings.py# vim crawlers/crawling/settings.py 修改以下 12345678910111213141516# Redis host configurationREDIS_HOST = '168.*.*.119'REDIS_PORT = 6379REDIS_DB = 0KAFKA_HOSTS = '168.*.*.119:9092'KAFKA_TOPIC_PREFIX = 'demo'KAFKA_CONN_TIMEOUT = 5KAFKA_APPID_TOPICS = FalseKAFKA_PRODUCER_BATCH_LINGER_MS = 25 # 25 ms before flushKAFKA_PRODUCER_BUFFER_BYTES = 4 * 1024 * 1024 # 4MB before blocking# Zookeeper SettingsZOOKEEPER_ASSIGN_PATH = '/scrapy-cluster/crawler/'ZOOKEEPER_ID = 'all'ZOOKEEPER_HOSTS = '168.*.*.119:2181' 启动监听 12# nohup python kafka_monitor.py run &gt;&gt; /root/scrapy-cluster/kafka-monitor/kafka_monitor.log 2&gt;&amp;1 &amp;# nohup python redis_monitor.py &gt;&gt; /root/scrapy-cluster/redis-monitor/redis_monitor.log 2&gt;&amp;1 &amp; scrapyd 爬虫管理工具配置 安装 1# pip install scrapyd 配置 12# sudo mkdir /etc/scrapyd# sudo vi /etc/scrapyd/scrapyd.conf 1234567891011121314151617181920212223242526272829[scrapyd]eggs_dir = eggslogs_dir = logsitems_dir =jobs_to_keep = 5dbs_dir = dbsmax_proc = 0max_proc_per_cpu = 10finished_to_keep = 100poll_interval = 5.0bind_address = 0.0.0.0http_port = 6800debug = offrunner = scrapyd.runnerapplication = scrapyd.app.applicationlauncher = scrapyd.launcher.Launcherwebroot = scrapyd.website.Root[services]schedule.json = scrapyd.webservice.Schedulecancel.json = scrapyd.webservice.Canceladdversion.json = scrapyd.webservice.AddVersionlistprojects.json = scrapyd.webservice.ListProjectslistversions.json = scrapyd.webservice.ListVersionslistspiders.json = scrapyd.webservice.ListSpidersdelproject.json = scrapyd.webservice.DeleteProjectdelversion.json = scrapyd.webservice.DeleteVersionlistjobs.json = scrapyd.webservice.ListJobsdaemonstatus.json = scrapyd.webservice.DaemonStatus 启动 1# nohup scrapyd &gt;&gt; /root/scrapy-cluster/scrapyd.log 2&gt;&amp;1 &amp; 建议做Nginx反向代理 启动异常 1234567891011121314File "/usr/local/lib/python3.6/site-packages/scrapyd-1.2.0-py3.6.egg/scrapyd/app.py", line 2, in &lt;module&gt;from twisted.application.internet import TimerService, TCPServerFile "/usr/local/lib64/python3.6/site-packages/twisted/application/internet.py", line 54, in &lt;module&gt;from automat import MethodicalMachineFile "/usr/local/lib/python3.6/site-packages/automat/__init__.py", line 2, in &lt;module&gt;from ._methodical import MethodicalMachineFile "/usr/local/lib/python3.6/site-packages/automat/_methodical.py", line 210, in &lt;module&gt; class MethodicalInput(object):File "/usr/local/lib/python3.6/site-packages/automat/_methodical.py", line 220, in MethodicalInput @argSpec.defaultbuiltins.TypeError: '_Nothing' object is not callableFailed to load application: '_Nothing' object is not callable 解决：Automat降级 1pip install Automat==0.6.0 Spiderkeeper 爬虫管理界面配置 安装 1pip install SpiderKeeper 启动 12mkdir /root/spiderkeeper/nohup spiderkeeper --server=http://168.*.*.118:6800 --username=admin --password=admin --database-url=sqlite:////root/spiderkeeper/SpiderKeeper.db &gt;&gt; /root/scrapy-cluster/spiderkeeper.log 2&gt;&amp;1 &amp; 浏览器访问http://168.*.*.118:5000 使用Spiderkeeper 管理爬虫使用scrapyd-deploy部署爬虫项目 修改scrapy.cfg配置 1vim /root/scrapy-cluster/crawler/scrapy.cfg 123456[settings]default = crawling.settings[deploy]url = http://168.*.*.118:6800/project = crawling 添加新的spider 1cd /root/scrapy-cluster/crawler/crawling/spider 使用scrapyd-deploy部署项目 123456# cd /root/scrapy-cluster/crawler# scrapyd-deploy Packing version 1536225989Deploying to project "crawling" in http://168.*.*.118:6800/addversion.jsonServer response (200):{"status": "ok", "project": "crawling", "version": "1536225989", "spiders": 3, "node_name": "ambari"} spiderkeeper 配置爬虫项目 登录Spiderkeeper创建项目 使用scrapy.cfg中配置的项目名创建后再Spiders-&gt;Dashboard中看到所有spider Scrapy-cluster 分布式爬虫Scrapy Cluster需要在不同的爬虫服务器之间进行协调，以确保最大的内容吞吐量，同时控制集群服务器爬取网站的速度。 Scrapy Cluster提供了两种主要策略来控制爬虫对不同域名的攻击速度。这由爬虫的类型与IP地址确定，但他们都作用于不同的域名队列。 Scrapy-cluster分布式爬虫，分发网址是基于IP地址。在不同的机器上启动集群，不同服务器上的每个爬虫去除队列中的所有链接。 部署集群中第二个scrapy-cluster配置一台新的服务器参照scrapy-cluster 单机配置,同时使用第一台服务器配置kafka-monitor/settings.py redis-monitor/settings.py crawling/settings.py Current public ip 问题由于两台服务器同时部署在相同内网，spider运行后即获取相同Current public ip，导致scrapy-cluster调度器无法根据IP分发链接 12018-09-07 16:08:29,684 [sc-crawler] DEBUG: Current public ip: b'110.90.122.1' 参考代码/root/scrapy-cluster/crawler/crawling/distributed_scheduler.py第282行： 1234567891011121314try: obj = urllib.request.urlopen(settings.get('PUBLIC_IP_URL', 'http://ip.42.pl/raw')) results = self.ip_regex.findall(obj.read()) if len(results) &gt; 0: # results[0] 获取IP地址即为110.90.122.1 self.my_ip = results[0] else: raise IOError("Could not get valid IP Address") obj.close() self.logger.debug("Current public ip: {ip}".format(ip=self.my_ip))except IOError: self.logger.error("Could not reach out to get public ip") pass 建议修改代码，获取本机IP 12self.my_ip = [(s.connect(('8.8.8.8', 53)), s.getsockname()[0], s.close()) for s in [socket.socket(socket.AF_INET, socket.SOCK_DGRAM)]][0][1] 运行分布式爬虫在两个scrapy-cluster中运行相同Spider 1execute(['scrapy', 'runspider', 'crawling/spiders/link_spider.py']) 使用python kafka_monitor.py feed投递多个链接，使用DEBUG即可观察到链接分配情况 使用SpiderKeeper管理分布式爬虫配置scrapyd管理集群第二个scrapy-cluster在第二台scrapy-cluster服务器上安装配置scrapyd，参考scrapyd 爬虫管理工具配置并修改配置 123456[settings]default = crawling.settings[deploy]url = http://168.*.*.119:6800/project = crawling 启动scrapyd后使用scrapyd-deploy工具部署两个scrapy-cluster上的爬虫项目。 使用Spiderkeeper连接多个scrapy-cluster 重新启动spiderkeeper，对接两个scrapy-cluster的管理工具scrapyd。 1nohup spiderkeeper --server=http://168.*.*.118:6800 --server=http://168.*.*.119:6800 --username=admin --password=admin --database-url=sqlite:////root/spiderkeeper/SpiderKeeper.db &gt;&gt; /root/scrapy-cluster/spiderkeeper.log 2&gt;&amp;1 &amp; 注意：要使用spiderkeeper管理同一个集群，爬虫项目名称必须一致，同时集群中scrapy-cluster配置相同spider任务 浏览器访问http://168.*.*.118:5000 启动爬虫时即可看见两个scrapy-cluster集群配置，启动同名爬虫开始scrapy-cluster分布式爬虫 启动分布式爬虫后状态]]></content>
  </entry>
</search>
