{"compress":true,"commitItems":[["cbb676ad-335e-4a6b-9409-be36614d0aa6",1537007116140,"---\ntitle: Scrapy-cluster分布式爬虫\ndate: 2018-09-10 20:45:21\ntags:\ncopyright: true\n---\n> 记录搭建scrapy-cluster以及管理工具scrapyd+spiderkeeper\n\n<!-- more -->\n\n - 基于Scrapy-cluster库的kafka-monitor可以实现分布式爬虫\n - Scrapyd+Spiderkeeper实现爬虫的可视化管理\n\n## 环境\n\n| IP | Role |\n| --- | --- |\n| 168.\\*.*.118 | Scrapy-cluster,scrapyd,spiderkeeper |\n| 168.\\*.*.119 | Scrapy-cluster,scrapyd,kafka,redis,zookeeper |\n\n```\n# cat /etc/redhat-release \nCentOS Linux release 7.4.1708 (Core) \n# python -V\nPython 2.7.5\n# java -version\nopenjdk version \"1.8.0_181\"\nOpenJDK Runtime Environment (build 1.8.0_181-b13)\nOpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)\n```\n\n## Zookeeper 单机配置\n\n* 下载并配置\n\n```\n# wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz\n# tar -zxvf zookeeper-3.4.13.tar.gz\n# cd zookeeper-3.4.13/conf\n# cp zoo_sample.cfg zoo.cfg\n# cd ..\n# PATH=/opt/zookeeper-3.4.13/bin:$PATH\n# echo 'export PATH=/opt/zookeeper-3.4.13/bin:$PATH' > /etc/profile.d/zoo.sh\n```\n\n* 单节点启动\n\n```\n# zkServer.sh status\nZooKeeper JMX enabled by default\nUsing config: /opt/zookeeper-3.4.13/bin/../conf/zoo.cfg\nError contacting service. It is probably not running.\n\n# zkServer.sh start\n```\n\n## kafka 单机配置\n* 下载\n\n```\n# wget http://mirrors.hust.edu.cn/apache/kafka/2.0.0/kafka_2.12-2.0.0.tgz\n# tar -zxvf kafka_2.12-2.0.0.tgz\n# cd kafka_2.12-2.0.0/\n```\n\n* 配置\n\n```sh\n# vim config/server.properties\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id=0                     # kafka的机器编号，\nhost.name = 168.*.*.119         # 绑定ip\nport=9092                        # 默认端口9092，\n# Switch to enable topic deletion or not, default value is false\ndelete.topic.enable=true\n############################# Zookeeper #############################\nzookeeper.connect=localhost:2181\n```\n* 启动\n\n```\nnohup bin/kafka-server-start.sh config/server.properties & \n```\n停止命令`bin/kafka-server-stop.sh config/server.properties`\n\n## redis 单机配置\n* 安装配置\n\n\n```\n# yum -y install redis\n# vim /etc/redis.conf\nbind 168.*.*.119\n```\n* 启动\n\n```\n# systemctl start redis.service\n```\n\n## scrapy-cluster 单机配置\n\n```\n# git clone https://github.com/istresearch/scrapy-cluster.git\n# cd scrapy-cluster\n# pip install -r requirements.txt\n```\n* 离线运行单元测试,以确保一切似乎正常\n\n```\n# ./run_offline_tests.sh\n```\n* 修改配置\n\n```\n# vim kafka-monitor/settings.py\n# vim redis-monitor/settings.py\n# vim crawlers/crawling/settings.py\n```\n* 修改以下\n\n```\n# Redis host configuration\nREDIS_HOST = '168.*.*.119'\nREDIS_PORT = 6379\nREDIS_DB = 0\n\nKAFKA_HOSTS = '168.*.*.119:9092'\nKAFKA_TOPIC_PREFIX = 'demo'\nKAFKA_CONN_TIMEOUT = 5\nKAFKA_APPID_TOPICS = False\nKAFKA_PRODUCER_BATCH_LINGER_MS = 25  # 25 ms before flush\nKAFKA_PRODUCER_BUFFER_BYTES = 4 * 1024 * 1024  # 4MB before blocking\n\n# Zookeeper Settings\nZOOKEEPER_ASSIGN_PATH = '/scrapy-cluster/crawler/'\nZOOKEEPER_ID = 'all'\nZOOKEEPER_HOSTS = '168.*.*.119:2181'\n```\n* 启动监听\n\n```\n# nohup python kafka_monitor.py run >> /root/scrapy-cluster/kafka-monitor/kafka_monitor.log 2>&1 &\n# nohup python redis_monitor.py >> /root/scrapy-cluster/redis-monitor/redis_monitor.log 2>&1 &\n```\n\n## scrapyd 爬虫管理工具配置\n* 安装\n\n```\n# pip install scrapyd\n```\n* 配置\n\n\n```\n# sudo mkdir /etc/scrapyd\n# sudo vi /etc/scrapyd/scrapyd.conf\n```\n\n```\n[scrapyd]\neggs_dir    = eggs\nlogs_dir    = logs\nitems_dir   =\njobs_to_keep = 5\ndbs_dir     = dbs\nmax_proc    = 0\nmax_proc_per_cpu = 10\nfinished_to_keep = 100\npoll_interval = 5.0\nbind_address = 0.0.0.0\nhttp_port   = 6800\ndebug       = off\nrunner      = scrapyd.runner\napplication = scrapyd.app.application\nlauncher    = scrapyd.launcher.Launcher\nwebroot     = scrapyd.website.Root\n\n[services]\nschedule.json     = scrapyd.webservice.Schedule\ncancel.json       = scrapyd.webservice.Cancel\naddversion.json   = scrapyd.webservice.AddVersion\nlistprojects.json = scrapyd.webservice.ListProjects\nlistversions.json = scrapyd.webservice.ListVersions\nlistspiders.json  = scrapyd.webservice.ListSpiders\ndelproject.json   = scrapyd.webservice.DeleteProject\ndelversion.json   = scrapyd.webservice.DeleteVersion\nlistjobs.json     = scrapyd.webservice.ListJobs\ndaemonstatus.json = scrapyd.webservice.DaemonStatus\n```\n\n* 启动\n\n```\n# nohup scrapyd >> /root/scrapy-cluster/scrapyd.log 2>&1 &\n```\n> 建议做Nginx反向代理\n\n* ***启动异常***\n\n```\nFile \"/usr/local/lib/python3.6/site-packages/scrapyd-1.2.0-py3.6.egg/scrapyd/app.py\", line 2, in <module>\nfrom twisted.application.internet import TimerService, TCPServer\nFile \"/usr/local/lib64/python3.6/site-packages/twisted/application/internet.py\", line 54, in <module>\nfrom automat import MethodicalMachine\nFile \"/usr/local/lib/python3.6/site-packages/automat/__init__.py\", line 2, in <module>\nfrom ._methodical import MethodicalMachine\nFile \"/usr/local/lib/python3.6/site-packages/automat/_methodical.py\", line 210, in <module>\n    class MethodicalInput(object):\nFile \"/usr/local/lib/python3.6/site-packages/automat/_methodical.py\", line 220, in MethodicalInput\n    @argSpec.default\nbuiltins.TypeError: '_Nothing' object is not callable\n\n\nFailed to load application: '_Nothing' object is not callable\n```\n* ***解决：Automat降级***\n\n```\npip install Automat==0.6.0\n```\n\n## Spiderkeeper 爬虫管理界面配置\n\n* 安装\n\n\n```\npip install SpiderKeeper\n```\n* 启动\n\n\n```\nmkdir /root/spiderkeeper/\nnohup spiderkeeper --server=http://168.*.*.118:6800 --username=admin --password=admin --database-url=sqlite:////root/spiderkeeper/SpiderKeeper.db >> /root/scrapy-cluster/spiderkeeper.log 2>&1 &\n```\n\n* 浏览器访问http://168.*.*.118:5000\n\n\n## 使用Spiderkeeper 管理爬虫\n### 使用scrapyd-deploy部署爬虫项目\n* 修改scrapy.cfg配置\n\n```\nvim /root/scrapy-cluster/crawler/scrapy.cfg\n```\n\n```\n[settings]\ndefault = crawling.settings\n\n[deploy]\nurl = http://168.*.*.118:6800/\nproject = crawling\n```\n\n* 添加新的spider\n\n\n```\ncd /root/scrapy-cluster/crawler/crawling/spider\n```\n\n* 使用scrapyd-deploy部署项目\n\n\n```\n# cd /root/scrapy-cluster/crawler\n# scrapyd-deploy \nPacking version 1536225989\nDeploying to project \"crawling\" in http://168.*.*.118:6800/addversion.json\nServer response (200):\n{\"status\": \"ok\", \"project\": \"crawling\", \"version\": \"1536225989\", \"spiders\": 3, \"node_name\": \"ambari\"}\n```\n### spiderkeeper 配置爬虫项目\n* 登录Spiderkeeper创建项目\n\n使用scrapy.cfg中配置的项目名\n![](media/15360542886109/15362341815397.jpg)\n创建后再Spiders->Dashboard中看到所有spider\n![](media/15360542886109/15362342524881.jpg)\n\n## Scrapy-cluster 分布式爬虫\n\nScrapy Cluster需要在不同的爬虫服务器之间进行协调，以确保最大的内容吞吐量，同时控制集群服务器爬取网站的速度。\n\nScrapy Cluster提供了两种主要策略来控制爬虫对不同域名的攻击速度。这由爬虫的类型与IP地址确定，但他们都作用于不同的域名队列。\n\nScrapy-cluster分布式爬虫，分发网址是基于IP地址。在不同的机器上启动集群，不同服务器上的每个爬虫去除队列中的所有链接。\n\n### 部署集群中第二个scrapy-cluster\n配置一台新的服务器参照[scrapy-cluster 单机配置](scrapy-cluster 单机配置),同时使用第一台服务器配置`kafka-monitor/settings.py` `redis-monitor/settings.py` `crawling/settings.py`\n\n### Current public ip 问题\n由于两台服务器同时部署在相同内网，spider运行后即获取相同`Current public ip`，导致scrapy-cluster调度器无法根据IP分发链接\n\n```\n2018-09-07 16:08:29,684 [sc-crawler] DEBUG: Current public ip: b'110.90.122.1'\n```\n参考代码`/root/scrapy-cluster/crawler/crawling/distributed_scheduler.py`第282行：\n\n```python\ntry:\n    obj = urllib.request.urlopen(settings.get('PUBLIC_IP_URL',\n                                  'http://ip.42.pl/raw'))\n    results = self.ip_regex.findall(obj.read())\n    if len(results) > 0:\n        # results[0] 获取IP地址即为110.90.122.1\n        self.my_ip = results[0]\n    else:\n        raise IOError(\"Could not get valid IP Address\")\n    obj.close()\n    self.logger.debug(\"Current public ip: {ip}\".format(ip=self.my_ip))\nexcept IOError:\n    self.logger.error(\"Could not reach out to get public ip\")\n    pass\n```\n建议修改代码，获取本机IP\n\n```python\nself.my_ip = [(s.connect(('8.8.8.8', 53)), s.getsockname()[0], s.close()) \n                for s in [socket.socket(socket.AF_INET, socket.SOCK_DGRAM)]][0][1]\n```\n\n### 运行分布式爬虫\n在两个scrapy-cluster中运行相同Spider\n\n```python\nexecute(['scrapy', 'runspider', 'crawling/spiders/link_spider.py'])\n```\n使用`python kafka_monitor.py feed`投递多个链接，使用DEBUG即可观察到链接分配情况\n\n## 使用SpiderKeeper管理分布式爬虫\n### 配置scrapyd管理集群第二个scrapy-cluster\n在第二台scrapy-cluster服务器上安装配置scrapyd，参考[scrapyd 爬虫管理工具配置](scrapyd 爬虫管理工具配置)并修改配置\n\n```\n[settings]\ndefault = crawling.settings\n\n[deploy]\nurl = http://168.*.*.119:6800/\nproject = crawling\n```\n启动scrapyd后使用scrapyd-deploy工具部署两个scrapy-cluster上的爬虫项目。\n\n### 使用Spiderkeeper连接多个scrapy-cluster\n\n* 重新启动spiderkeeper，对接两个scrapy-cluster的管理工具scrapyd。\n\n```\nnohup spiderkeeper --server=http://168.*.*.118:6800 --server=http://168.*.*.119:6800 --username=admin --password=admin --database-url=sqlite:////root/spiderkeeper/SpiderKeeper.db >> /root/scrapy-cluster/spiderkeeper.log 2>&1 &\n```\n\n> 注意：要使用spiderkeeper管理同一个集群，爬虫项目名称必须一致，同时集群中scrapy-cluster配置相同spider任务\n\n* 浏览器访问http://168.*.*.118:5000 启动爬虫时即可看见两个scrapy-cluster集群配置，启动同名爬虫开始scrapy-cluster分布式爬虫\n![](media/15360542886109/15363140725100.jpg)\n* 启动分布式爬虫后状态\n![](media/15360542886109/15363142039648.jpg)\n",[[1537007111204,["melkor@192.168.0.102",[[1,2931,"# #"]],[2996,2996],[3031,3031]]],[1537007118031,["melkor@192.168.0.102",[[-1,2931,"# #"]],[3031,3031],[2996,2996]]],[1537011293936,["melkor@192.168.0.102",[[1,6203,"\n| item      | Model    |  Price | Qty |\n| --------- | -------- | -----: | --: |\n| Laptop    | 13\" Pro  | $1,300 |   1 |\n| Phone     | Plus     |   $800 |   2 |\n| Watch     | Series 3 |   $400 |   3 |\n| Headphone | HD650    |   $350 |   2 |\n\n"]],[6202,6202],[6445,6445]]],[1537011301401,["melkor@192.168.0.102",[[-1,6203,"\n| item      | Model    |  Price | Qty |\n| --------- | -------- | -----: | --: |\n| Laptop    | 13\" Pro  | $1,300 |   1 |\n| Phone     | Plus     |   $800 |   2 |\n| Watch     | Series 3 |   $400 |   3 |\n| Headphone | HD650    |   $350 |   2 |\n\n"]],[6445,6445],[6202,6202]]]],null,"melkor@192.168.0.102"],["e6337632-9656-4487-96ee-547a0818a0b4",1537012245149,"---\ntitle: Scrapy-cluster分布式爬虫\ndate: 2018-09-10 20:45:21\ntags:\ncopyright: true\n---\n> 记录搭建scrapy-cluster以及管理工具scrapyd+spiderkeeper\n\n<!-- more -->\n\n - 基于Scrapy-cluster库的kafka-monitor可以实现分布式爬虫\n - Scrapyd+Spiderkeeper实现爬虫的可视化管理\n\n## 环境\n\n| IP | Role |\n| --- | --- |\n| 168.\\*.*.118 | Scrapy-cluster,scrapyd,spiderkeeper |\n| 168.\\*.*.119 | Scrapy-cluster,scrapyd,kafka,redis,zookeeper |\n\n```\n# cat /etc/redhat-release \nCentOS Linux release 7.4.1708 (Core) \n# python -V\nPython 2.7.5\n# java -version\nopenjdk version \"1.8.0_181\"\nOpenJDK Runtime Environment (build 1.8.0_181-b13)\nOpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)\n```\n\n## Zookeeper 单机配置\n\n* 下载并配置\n\n```\n# wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz\n# tar -zxvf zookeeper-3.4.13.tar.gz\n# cd zookeeper-3.4.13/conf\n# cp zoo_sample.cfg zoo.cfg\n# cd ..\n# PATH=/opt/zookeeper-3.4.13/bin:$PATH\n# echo 'export PATH=/opt/zookeeper-3.4.13/bin:$PATH' > /etc/profile.d/zoo.sh\n```\n\n* 单节点启动\n\n```\n# zkServer.sh status\nZooKeeper JMX enabled by default\nUsing config: /opt/zookeeper-3.4.13/bin/../conf/zoo.cfg\nError contacting service. It is probably not running.\n\n# zkServer.sh start\n```\n\n## kafka 单机配置\n* 下载\n\n```\n# wget http://mirrors.hust.edu.cn/apache/kafka/2.0.0/kafka_2.12-2.0.0.tgz\n# tar -zxvf kafka_2.12-2.0.0.tgz\n# cd kafka_2.12-2.0.0/\n```\n\n* 配置\n\n```sh\n# vim config/server.properties\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id=0                     # kafka的机器编号，\nhost.name = 168.*.*.119         # 绑定ip\nport=9092                        # 默认端口9092，\n# Switch to enable topic deletion or not, default value is false\ndelete.topic.enable=true\n############################# Zookeeper #############################\nzookeeper.connect=localhost:2181\n```\n* 启动\n\n```\nnohup bin/kafka-server-start.sh config/server.properties & \n```\n停止命令`bin/kafka-server-stop.sh config/server.properties`\n\n## redis 单机配置\n* 安装配置\n\n\n```\n# yum -y install redis\n# vim /etc/redis.conf\nbind 168.*.*.119\n```\n* 启动\n\n```\n# systemctl start redis.service\n```\n\n## scrapy-cluster 单机配置\n\n```\n# git clone https://github.com/istresearch/scrapy-cluster.git\n# cd scrapy-cluster\n# pip install -r requirements.txt\n```\n* 离线运行单元测试,以确保一切似乎正常\n\n```\n# ./run_offline_tests.sh\n```\n* 修改配置\n\n```\n# vim kafka-monitor/settings.py\n# vim redis-monitor/settings.py\n# vim crawlers/crawling/settings.py\n```\n* 修改以下\n\n```\n# Redis host configuration\nREDIS_HOST = '168.*.*.119'\nREDIS_PORT = 6379\nREDIS_DB = 0\n\nKAFKA_HOSTS = '168.*.*.119:9092'\nKAFKA_TOPIC_PREFIX = 'demo'\nKAFKA_CONN_TIMEOUT = 5\nKAFKA_APPID_TOPICS = False\nKAFKA_PRODUCER_BATCH_LINGER_MS = 25  # 25 ms before flush\nKAFKA_PRODUCER_BUFFER_BYTES = 4 * 1024 * 1024  # 4MB before blocking\n\n# Zookeeper Settings\nZOOKEEPER_ASSIGN_PATH = '/scrapy-cluster/crawler/'\nZOOKEEPER_ID = 'all'\nZOOKEEPER_HOSTS = '168.*.*.119:2181'\n```\n* 启动监听\n\n```\n# nohup python kafka_monitor.py run >> /root/scrapy-cluster/kafka-monitor/kafka_monitor.log 2>&1 &\n# nohup python redis_monitor.py >> /root/scrapy-cluster/redis-monitor/redis_monitor.log 2>&1 &\n```\n\n## scrapyd 爬虫管理工具配置\n* 安装\n\n```\n# pip install scrapyd\n```\n* 配置\n\n\n```\n# sudo mkdir /etc/scrapyd\n# sudo vi /etc/scrapyd/scrapyd.conf\n```\n\n```\n[scrapyd]\neggs_dir    = eggs\nlogs_dir    = logs\nitems_dir   =\njobs_to_keep = 5\ndbs_dir     = dbs\nmax_proc    = 0\nmax_proc_per_cpu = 10\nfinished_to_keep = 100\npoll_interval = 5.0\nbind_address = 0.0.0.0\nhttp_port   = 6800\ndebug       = off\nrunner      = scrapyd.runner\napplication = scrapyd.app.application\nlauncher    = scrapyd.launcher.Launcher\nwebroot     = scrapyd.website.Root\n\n[services]\nschedule.json     = scrapyd.webservice.Schedule\ncancel.json       = scrapyd.webservice.Cancel\naddversion.json   = scrapyd.webservice.AddVersion\nlistprojects.json = scrapyd.webservice.ListProjects\nlistversions.json = scrapyd.webservice.ListVersions\nlistspiders.json  = scrapyd.webservice.ListSpiders\ndelproject.json   = scrapyd.webservice.DeleteProject\ndelversion.json   = scrapyd.webservice.DeleteVersion\nlistjobs.json     = scrapyd.webservice.ListJobs\ndaemonstatus.json = scrapyd.webservice.DaemonStatus\n```\n\n* 启动\n\n```\n# nohup scrapyd >> /root/scrapy-cluster/scrapyd.log 2>&1 &\n```\n> 建议做Nginx反向代理\n\n* ***启动异常***\n\n```\nFile \"/usr/local/lib/python3.6/site-packages/scrapyd-1.2.0-py3.6.egg/scrapyd/app.py\", line 2, in <module>\nfrom twisted.application.internet import TimerService, TCPServer\nFile \"/usr/local/lib64/python3.6/site-packages/twisted/application/internet.py\", line 54, in <module>\nfrom automat import MethodicalMachine\nFile \"/usr/local/lib/python3.6/site-packages/automat/__init__.py\", line 2, in <module>\nfrom ._methodical import MethodicalMachine\nFile \"/usr/local/lib/python3.6/site-packages/automat/_methodical.py\", line 210, in <module>\n    class MethodicalInput(object):\nFile \"/usr/local/lib/python3.6/site-packages/automat/_methodical.py\", line 220, in MethodicalInput\n    @argSpec.default\nbuiltins.TypeError: '_Nothing' object is not callable\n\n\nFailed to load application: '_Nothing' object is not callable\n```\n* ***解决：Automat降级***\n\n```\npip install Automat==0.6.0\n```\n\n## Spiderkeeper 爬虫管理界面配置\n\n* 安装\n\n\n```\npip install SpiderKeeper\n```\n* 启动\n\n\n```\nmkdir /root/spiderkeeper/\nnohup spiderkeeper --server=http://168.*.*.118:6800 --username=admin --password=admin --database-url=sqlite:////root/spiderkeeper/SpiderKeeper.db >> /root/scrapy-cluster/spiderkeeper.log 2>&1 &\n```\n\n* 浏览器访问http://168.*.*.118:5000\n\n\n## 使用Spiderkeeper 管理爬虫\n### 使用scrapyd-deploy部署爬虫项目\n* 修改scrapy.cfg配置\n\n```\nvim /root/scrapy-cluster/crawler/scrapy.cfg\n```\n\n```\n[settings]\ndefault = crawling.settings\n\n[deploy]\nurl = http://168.*.*.118:6800/\nproject = crawling\n```\n\n* 添加新的spider\n\n\n```\ncd /root/scrapy-cluster/crawler/crawling/spider\n```\n\n* 使用scrapyd-deploy部署项目\n\n\n```\n# cd /root/scrapy-cluster/crawler\n# scrapyd-deploy \nPacking version 1536225989\nDeploying to project \"crawling\" in http://168.*.*.118:6800/addversion.json\nServer response (200):\n{\"status\": \"ok\", \"project\": \"crawling\", \"version\": \"1536225989\", \"spiders\": 3, \"node_name\": \"ambari\"}\n```\n### spiderkeeper 配置爬虫项目\n* 登录Spiderkeeper创建项目\n\n使用scrapy.cfg中配置的项目名\n![](media/15360542886109/15362341815397.jpg)\n创建后再Spiders->Dashboard中看到所有spider\n![](media/15360542886109/15362342524881.jpg)\n\n## Scrapy-cluster 分布式爬虫\n\nScrapy Cluster需要在不同的爬虫服务器之间进行协调，以确保最大的内容吞吐量，同时控制集群服务器爬取网站的速度。\n\nScrapy Cluster提供了两种主要策略来控制爬虫对不同域名的攻击速度。这由爬虫的类型与IP地址确定，但他们都作用于不同的域名队列。\n\nScrapy-cluster分布式爬虫，分发网址是基于IP地址。在不同的机器上启动集群，不同服务器上的每个爬虫去除队列中的所有链接。\n\n### 部署集群中第二个scrapy-cluster\n配置一台新的服务器参照[scrapy-cluster 单机配置](scrapy-cluster 单机配置),同时使用第一台服务器配置`kafka-monitor/settings.py` `redis-monitor/settings.py` `crawling/settings.py`\n\n### Current public ip 问题\n由于两台服务器同时部署在相同内网，spider运行后即获取相同`Current public ip`，导致scrapy-cluster调度器无法根据IP分发链接\n\n```\n2018-09-07 16:08:29,684 [sc-crawler] DEBUG: Current public ip: b'110.90.122.1'\n```\n参考代码`/root/scrapy-cluster/crawler/crawling/distributed_scheduler.py`第282行：\n\n```python\ntry:\n    obj = urllib.request.urlopen(settings.get('PUBLIC_IP_URL',\n                                  'http://ip.42.pl/raw'))\n    results = self.ip_regex.findall(obj.read())\n    if len(results) > 0:\n        # results[0] 获取IP地址即为110.90.122.1\n        self.my_ip = results[0]\n    else:\n        raise IOError(\"Could not get valid IP Address\")\n    obj.close()\n    self.logger.debug(\"Current public ip: {ip}\".format(ip=self.my_ip))\nexcept IOError:\n    self.logger.error(\"Could not reach out to get public ip\")\n    pass\n```\n建议修改代码，获取本机IP\n\n```python\nself.my_ip = [(s.connect(('8.8.8.8', 53)), s.getsockname()[0], s.close()) \n                for s in [socket.socket(socket.AF_INET, socket.SOCK_DGRAM)]][0][1]\n```\n\n### 运行分布式爬虫\n在两个scrapy-cluster中运行相同Spider\n\n```python\nexecute(['scrapy', 'runspider', 'crawling/spiders/link_spider.py'])\n```\n使用`python kafka_monitor.py feed`投递多个链接，使用DEBUG即可观察到链接分配情况\n\n## 使用SpiderKeeper管理分布式爬虫\n### 配置scrapyd管理集群第二个scrapy-cluster\n在第二台scrapy-cluster服务器上安装配置scrapyd，参考[scrapyd 爬虫管理工具配置](scrapyd 爬虫管理工具配置)并修改配置\n\n```\n[settings]\ndefault = crawling.settings\n\n[deploy]\nurl = http://168.*.*.119:6800/\nproject = crawling\n```\n启动scrapyd后使用scrapyd-deploy工具部署两个scrapy-cluster上的爬虫项目。\n\n### 使用Spiderkeeper连接多个scrapy-cluster\n\n* 重新启动spiderkeeper，对接两个scrapy-cluster的管理工具scrapyd。\n\n```\nnohup spiderkeeper --server=http://168.*.*.118:6800 --server=http://168.*.*.119:6800 --username=admin --password=admin --database-url=sqlite:////root/spiderkeeper/SpiderKeeper.db >> /root/scrapy-cluster/spiderkeeper.log 2>&1 &\n```\n\n> 注意：要使用spiderkeeper管理同一个集群，爬虫项目名称必须一致，同时集群中scrapy-cluster配置相同spider任务\n\n* 浏览器访问http://168.*.*.118:5000 启动爬虫时即可看见两个scrapy-cluster集群配置，启动同名爬虫开始scrapy-cluster分布式爬虫\n![](media/15360542886109/15363140725100.jpg)\n* 启动分布式爬虫后状态\n![](media/15360542886109/15363142039648.jpg)\n",[[1537012213493,["melkor@192.168.0.102",[[1,6158,"\n"]],[6157,6157],[6158,6158]]],[1537012912931,["melkor@192.168.0.102",[[-1,6158,"\n"]],[6158,6158],[6157,6157]]],[1537013004376,["melkor@192.168.0.102",[[1,6158,"[itm_custom_script](images/itm_custom_script.img)\n"]],[6157,6157],[6208,6208]]],[1537013016234,["melkor@192.168.0.102",[[1,6158,"!"]],[6158,6158],[6159,6159]]],[1537013527819,["melkor@192.168.0.102",[[1,7546,"  "]],[7546,7546],[7548,7548]]],[1537013531111,["melkor@192.168.0.102",[[-1,7546,"  "]],[7548,7548],[7546,7546]]],[1537014129358,["melkor@192.168.0.102",[[-1,8700,"15360542886109/"]],[8702,8714],[8699,8699]]],[1537014138703,["melkor@192.168.0.102",[[1,8694,"、"]],[8694,8694],[8695,8695]]],[1537014139604,["melkor@192.168.0.102",[[-1,8694,"、"]],[8695,8695],[8694,8694]]],[1537014140068,["melkor@192.168.0.102",[[1,8694,"/"]],[8694,8694],[8695,8695]]],[1537014149123,["melkor@192.168.0.102",[[-1,8694,"/"]],[8695,8695],[8694,8694]]],[1537014167268,["melkor@192.168.0.102",[[1,8694,"i"]],[8694,8694],[8695,8695]]],[1537014167701,["melkor@192.168.0.102",[[1,8696,"m"]],[8695,8695],[8696,8696]]],[1537014168214,["melkor@192.168.0.102",[[1,8696,"a"]],[8696,8696],[8697,8697]]],[1537014169379,["melkor@192.168.0.102",[[-1,8694,"iam"]],[8697,8697],[8694,8694]]],[1537014171017,["melkor@192.168.0.102",[[1,8694,"/i"]],[8694,8694],[8696,8696]]],[1537014171462,["melkor@192.168.0.102",[[1,8697,"m"]],[8696,8696],[8697,8697]]],[1537014172627,["melkor@192.168.0.102",[[1,8697,"ages/"]],[8697,8697],[8702,8702]]],[1537014197311,["melkor@192.168.0.102",[[-1,6158,"![itm_custom_script](images/itm_custom_script.img)"]],[6158,6208],[6158,6158]]],[1537014200043,["melkor@192.168.0.102",[[-1,6158,"\n"]],[6158,6158],[6157,6157]]],[1537014203988,["melkor@192.168.0.102",[[-1,8643,"/"]],[8644,8644],[8643,8643]]],[1537014211858,["melkor@192.168.0.102",[[-1,8699,"15360542886109/"]],[8700,8713],[8698,8698]]],[1537014215318,["melkor@192.168.0.102",[[1,8693,"i"]],[8693,8693],[8694,8694]]],[1537014215571,["melkor@192.168.0.102",[[1,8695,"m"]],[8694,8694],[8695,8695]]],[1537014217162,["melkor@192.168.0.102",[[1,8695,"ages/"]],[8695,8695],[8700,8700]]],[1537014243644,["melkor@192.168.0.102",[[-1,6168,"15360542886109/"]],[6170,6182],[6167,6167]]],[1537014245971,["melkor@192.168.0.102",[[-1,6232,"15360542886109/"]],[6233,6246],[6231,6231]]],[1537014247777,["melkor@192.168.0.102",[[1,6162,"i"]],[6162,6162],[6163,6163]]],[1537014247963,["melkor@192.168.0.102",[[1,6164,"m"]],[6163,6163],[6164,6164]]],[1537014248758,["melkor@192.168.0.102",[[1,6164,"ages/"]],[6164,6164],[6169,6169]]],[1537014251033,["melkor@192.168.0.102",[[1,6233,"i"]],[6233,6233],[6234,6234]]],[1537014251211,["melkor@192.168.0.102",[[1,6235,"m"]],[6234,6234],[6235,6235]]],[1537014252025,["melkor@192.168.0.102",[[1,6235,"ages;"]],[6235,6235],[6240,6240]]],[1537014252624,["melkor@192.168.0.102",[[-1,6239,";"]],[6240,6240],[6239,6239]]],[1537014253257,["melkor@192.168.0.102",[[1,6239,"/"]],[6239,6239],[6240,6240]]],[1537085274414,["melkor@192.168.0.102",[[-1,131,"<!-- more -->"]],[131,131],[131,131]]],[1537085275343,["melkor@192.168.0.102",[[1,131,"<!-- more -->"]],[131,131],[131,131]]],[1537085276005,["melkor@192.168.0.102",[[-1,6239,"/"]],[6240,6240],[6239,6239]]],[1537085277468,["melkor@192.168.0.102",[[1,6239,";"]],[6239,6239],[6240,6240]]],[1537085279715,["melkor@192.168.0.102",[[-1,6235,"ages;m"]],[6240,6240],[6234,6234]]],[1537087226981,["melkor@192.168.0.102",[[-1,6233,"i"]],[6234,6234],[6233,6233]]],[1537087229659,["melkor@192.168.0.102",[[-1,6162,"iages/m"]],[6169,6169],[6162,6162]]],[1537087230351,["melkor@192.168.0.102",[[1,6232,"15360542886109/"]],[6231,6231],[6233,6246]]],[1537087232608,["melkor@192.168.0.102",[[-1,6232,"15360542886109/"]],[6233,6246],[6231,6231]]],[1537097725056,["melkor@192.168.0.102",[[1,6231,"\t"]],[6231,6231],[6232,6232]]],[1537097729182,["melkor@192.168.0.102",[[-1,6231,"\t"]],[6232,6232],[6231,6231]]]],null,"melkor@192.168.0.102"]]}